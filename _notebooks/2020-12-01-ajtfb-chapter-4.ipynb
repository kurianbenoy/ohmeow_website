{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-11-06-ajtfb-chapter-1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWJL4-Qwluy4"
      },
      "source": [
        "# \"A Journey Through Fastbook (AJTFB) - Chapter 4\"\n",
        "> \"The fourth in a weekly-ish series where I revisit the fast.ai book, [\\\"Deep Learning for Coders with fastai & PyTorch\\\"](https://github.com/fastai/fastbook), and provide commentary on the bits that jumped out to me chapter by chapter.  So without further adieu, let's go!\"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- badges: true\n",
        "- hide_binder_badge: true\n",
        "- comments: false\n",
        "- author: Wayde Gilliam\n",
        "- categories: [fastai, fastbook]\n",
        "- image: images/articles/fastbook.jpg\n",
        "- hide: true\n",
        "- search_exclude: true\n",
        "- permalink: temp-posts/ajtfb-chapter4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfcRNESotGLU"
      },
      "source": [
        "Other posts in this series:  \n",
        "[A Journey Through Fastbook (AJTFB) - Chapter 1](https://ohmeow.com/posts/2020/11/06/_11_06_ajtfb_chapter_1.html)  \n",
        "[A Journey Through Fastbook (AJTFB) - Chapter 2](https://ohmeow.com/posts/2020/11/16/ajtfb-chapter-2.html)  \n",
        "[A Journey Through Fastbook (AJTFB) - Chapter 3](https://ohmeow.com/posts/2020/11/22/ajtfb-chapter-3.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWxCOulE-ACJ"
      },
      "source": [
        "!pip install fastai>=2.3 --upgrade\n",
        "\n",
        "import fastai\n",
        "from fastai.vision.all import *\n",
        "print(fastai.__version__ )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyL36r-q-MRO"
      },
      "source": [
        "## Chapter 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn6OyriNApwH"
      },
      "source": [
        "---\n",
        "### How to visualize a grayscale image in pandas ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWRBzg9Z-Lof"
      },
      "source": [
        "mnist_path = untar_data(URLs.MNIST_SAMPLE); mnist_path.ls()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNW-l1sL_l0l"
      },
      "source": [
        "sample_3 = Image.open((mnist_path/'train/3').ls().sorted()[1])\n",
        "sample_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaesatM_AAiI"
      },
      "source": [
        "sample_3_t = tensor(sample_3)\n",
        "df = pd.DataFrame(sample_3_t[4:15, 4:22])\n",
        "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoBHs0wwbqG6"
      },
      "source": [
        "---\n",
        "### What is a baseline model and why do you want one?\n",
        "\n",
        "> A simple model that you are confident should perform reasonably well. It should be\n",
        "> simple to implement and easy to test\n",
        "\n",
        "**Why do you want to start with a baseline model?**\n",
        "> ... without starting with a sensible baseline, it is difficult to know whether your super-fancy models are any good\n",
        "\n",
        "**How do you build/find one of these models?**\n",
        "\n",
        "You can search online for folks that have trained models to solve a problem similar to your's and/or you can start with one of the high-level examples in the fastai docs against your data.  There are a bunch covering core vision, text, tabuluar and colab filtering tasks right [here](https://docs.fast.ai/tutorial.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x904qZCHHcb"
      },
      "source": [
        "---\n",
        "### Tensors\n",
        "\n",
        "**What is a \"Tensor\"?**\n",
        "\n",
        "Like a numpy array, but with GPU support.  The data it contains must be of the ***same type*** and must conform in ***rectangular shape***.\n",
        "\n",
        "> Important: \"try to avoid as much as possible writing loops, and replace them by commands that work directly on arrays or tensors\"\n",
        "\n",
        "Let's take a look .."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4So8iq5hAd7u"
      },
      "source": [
        "threes = (mnist_path/'train/3').ls().sorted()\n",
        "len(threes), threes[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNHfEYsHBpSF"
      },
      "source": [
        "all_threes = [ tensor(Image.open(fp)) for fp in threes ]\n",
        "len(all_threes), all_threes[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df-v9cdXBpOE"
      },
      "source": [
        "stacked_threes = torch.stack(all_threes).float()/255\n",
        "stacked_threes.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDvJ0vpCC01_"
      },
      "source": [
        "Important information about tensors include its `shape`, `rank`, and `type`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcbS_oJBCsiJ"
      },
      "source": [
        "# shape = the length of each axis\n",
        "print('shape: ', stacked_threes.shape)\n",
        "\n",
        "# rank = the total number of axes\n",
        "print('rank: ', stacked_threes.ndim)\n",
        "\n",
        "# type = the datatype of its contents\n",
        "print('type: ', stacked_threes.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow5Fn9hvFrSZ"
      },
      "source": [
        "Check out pp.145-148 to learn about \"broadcasting\", a critical piece to understanding how you can and should manipulate tensors or numpy arrays!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yYHVT5kGXFc"
      },
      "source": [
        "---\n",
        "### Stochastic Gradient Descent - How to train a model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpqbW9GgqrZN"
      },
      "source": [
        "Here are the steps:\n",
        "\n",
        "1. **INITIALIZE** the weights = initializing parameters to random values\n",
        "\n",
        "2. For each image, **PREDICT** whether it is a 3 or 7\n",
        " \n",
        "3. Based on the predictions, calculate how good the model is by calculating its **LOSS** (small is good)\n",
        "\n",
        "4. Calculate the **GRADIENT**, ***\"which measures for each weight how changing the weight would change the loss\"***\n",
        "\n",
        "5. **STEP**, change all the weights based on the gradient\n",
        "\n",
        "6. Starting at step 2, **REPEAT**\n",
        "\n",
        "7. **STOP** when you don't want to train any longer or the model is good enough\n",
        "\n",
        "Below, we'll delve deeper into steps 4 and 5 since how they work are likely the most foreign. We'll do this by getting a big more into the sample code beginning on p.150 ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv7RlblLIwnn"
      },
      "source": [
        "### Step 4: Calculating the gradients\n",
        "\n",
        "> Important: \"the gradients ***tell us how much we have to change each weight*** to make our model better ... allows us to more quickly calculate whether our loss will go up or down we we make those adjustments\"\n",
        "\n",
        "\n",
        "> Important: \"The gradients ***tell us only the slope of our function***; they don't tell us exactly how far to adjust the parameters. But they do give us some idea of how far\" (large slope = bigger adjustments needed whereas a small slope suggests we are close to the optimal value)\n",
        "\n",
        "\n",
        "\"The ***derivative*** of a function tells you how much a change in its parameters will change its result\"\n",
        "\n",
        "Remember: We are calculating a gradient for *EVERY* weight so we know how to adjust it to make our model better (i.e., lower the LOSS)\n",
        "\n",
        "`requires_grad` tells PyTorch \"that we want to calculate gradients with respect to that variable at that value\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlKI_4ZWZWor"
      },
      "source": [
        "def plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)):\n",
        "    x = torch.linspace(min,max)\n",
        "    fig,ax = plt.subplots(figsize=figsize)\n",
        "    ax.plot(x,f(x))\n",
        "    if tx is not None: ax.set_xlabel(tx)\n",
        "    if ty is not None: ax.set_ylabel(ty)\n",
        "    if title is not None: ax.set_title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRRz6Z39Z8_I"
      },
      "source": [
        "Here we pretend that the below is our **loss function**.  Running a number through it, our **weight** will produce a result, an **activation** ... in this case, our **loss** (which again is a value telling us how good or bad our model is; smaller = good)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjQCRo2eGVpA"
      },
      "source": [
        "xt = tensor(-1.5).requires_grad_(); xt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpWRqpteY6dJ"
      },
      "source": [
        "def f(x): return x**2\n",
        "loss = f(xt)\n",
        "\n",
        "plot_function(f, 'x', 'x**2')\n",
        "plt.scatter(xt.detach().numpy(), loss.detach().numpy(), color='red')\n",
        "print('Loss: ', loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrvb8INMbEIy"
      },
      "source": [
        "So if our parameter is `-1.5` we get a loss = `2.25`. Since the direction of our slope is downward (negative), by changing its value to be a bit more positive, we get closer to achieving our goal of *minimizing our loss*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zb4dLlMcJMD"
      },
      "source": [
        "xt = tensor(-1.).requires_grad_(); xt\n",
        "\n",
        "loss = f(xt)\n",
        "\n",
        "plot_function(f, 'x', 'x**2')\n",
        "plt.scatter(xt.detach().numpy(), loss.detach().numpy(), color='red')\n",
        "print('Loss: ', loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HPZrsmdcd9x"
      },
      "source": [
        "And yes, our loss has improved!  If the direction of our slope were upwards (positive), we would conversely want `x` to be smaller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGv43ojRdVkE"
      },
      "source": [
        "***BUT*** now ... imagine having to figure all this out for a million parameters.  Obviously, we wouldn't want to try doing this manually as we did before, and thanks to PyTorch, we don't have too :)\n",
        "\n",
        "Remember that by utilizing the `requires_grad_()` function, we have told PyTorch to keep track of how to compute the gradients based on the other calucations we perform, like running it through our loss function above.  Let's see what that looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9YLALQ7dVR1"
      },
      "source": [
        "xt = tensor(-1.).requires_grad_(); \n",
        "print(xt)\n",
        "\n",
        "loss = f(xt)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIgLnDoqeXUY"
      },
      "source": [
        "That `<PowBackward0>` is the gradient function it will use to calculate the gradients when needed.  And when we need it, we call the `backward` method to do so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA1tgKFzeh5d"
      },
      "source": [
        "loss.backward()\n",
        "print(xt.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyCreCbhe2EP"
      },
      "source": [
        "And the calcuated gradient is exactly what we expected given that to calculate the derivate of `x**2` is `2x` ... `2*-1 = -2`.\n",
        "\n",
        "Again, the gradient tells us ***the slope of our function***.  Here have a a negative/downward slope and so at the very least, we know what moving in that direction will get us closer to the minimum.\n",
        "\n",
        "The question is now, **How far do we move in that direction?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1RePernMIPn"
      },
      "source": [
        "---\n",
        "### Step 5: Change all the weights based on the gradient using a \"Learning Rate\"\n",
        "\n",
        "The **learning rate** (or LR) is a number (usually a small number like 1e-3 or 0.1) that we multiply the gradient by to get a better parameter value.  For a given parameter/weight `w`, the calculation looks like this:\n",
        "\n",
        "` w -= w.grad * lr`\n",
        "\n",
        "Notice we take the negative of the grad * lr operation because we want to move in the opposite direction.\n",
        "\n",
        "> Important: We do this in a `with torch.no_grad()` so that we don't calculate the gradient for the gradient calculating operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpproXJdodrR"
      },
      "source": [
        "lr = 0.01\n",
        "\n",
        "with torch.no_grad():\n",
        " xt -= xt.grad * lr\n",
        "\n",
        " print('New value for xt: ', xt)\n",
        " print('New loss: ', f(xt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etZcxIkWoerO"
      },
      "source": [
        "You can see the loss get smaller which is exactly what we want!\n",
        "\n",
        "The above operation is also called the **optimization step**\n",
        "\n",
        "See pp.156-157 for examples of what using a too small or too large LR might look like when training.  This could help you troubleshoot things if yours looks wonky."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2uxHGE5IK0a"
      },
      "source": [
        "---\n",
        "### Measuring distances\n",
        "\n",
        "See pp.141-142.  There are two main ways to measure distances.\n",
        "\n",
        "**L1 norm** (or mean absolute difference): Take the mean of the absolute value of differences\n",
        "\n",
        "` l1_loss = (tensor_a - tensor_b).abs().mean()`\n",
        "\n",
        "**L2 norm** (or root mean squared error, RMSE): Take the square root of the mean of the square differences. The squaring of differences makes everything positive and the square root undoes the squaring.\n",
        "\n",
        "> Important: \"... the latter will penalize bigger mistakes more heavily than the former (and be more lenient with small mistakes)\"\n",
        "\n",
        "` l2_loss = ((tensor_a - tensor_b) ** 2).sqrt()`\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7ZF3BdAItCG"
      },
      "source": [
        "---\n",
        "### Disinformation\n",
        "\n",
        "> It is not necessarily about getting someone to belive something false, but rather often used to sow disharmony and uncertainty, and to get people to give up on seeking the truth. Receiving conflicting accounts can lead people to assume that they can never know whom or what to trust.\n",
        "\n",
        "Disinformation will unfotunately be one of the greatest legacies of President Trump. A step backwards for American society. A culture that will back if you if you tell them what they want to hear, even if you're a compulsive liar and base your statements on \"gut feel\" rather than facts and logic.\n",
        "\n",
        "> While most of us like to think of ourselves as independent-minded, in reality we evolved to be influenced by others in our in-group, and in opposition to those in our out-group. Online discussions can influence our viewpoints, or alter the range of what we consider acceptable viewpoints. Humans are social animals, and as social animals, we are extremely influenced by the people around us. Increasingly, radicalization occurs in online environments; so influence is coming from people in the virtual space of online forums and social networks.\n",
        "\n",
        "The biggest take here is that I am not as independently minded as I think I am. Knowing thyself is perhaps the best preventative of being swallowed up by disinformation. Limiting social media is another.\n",
        "\n",
        "> Disinformation through autogenerated text is a particularly significant issue\n",
        "\n",
        "As an NLP guy, this one scares me since part of my work is to summarize text. Knowing this, the first step I've taken is to let all business owners know the risk of text generation algorithms generating text that is either false and/or not necessarily reflective of the inputs, as in the case of abstract summarization.  The second step I took was to introduce human beings into the process and a workflow that has them look at at least the most potentially wrong summarizations before reports go out.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZoR3dj_Tu6C"
      },
      "source": [
        "---\n",
        "### What to do???\n",
        "\n",
        "> You must assuem that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal.\n",
        "\n",
        "Data use and storage are things you need to think about.\n",
        "\n",
        "I think these are good questions to ask/answer in any project to ensure good outcomes:\n",
        "\n",
        "> * Whose interests, desires, skills, experiences, and values have we simply assumed rather than\n",
        "> actually consulted?\n",
        "> * Who are all the stakeholders who will be directly affected by our product? How have their interests\n",
        "> been protected? How do we know what their interests really are - have we asked?\n",
        "> * Whowhich groups and individuals will be indirectly affected in signficant ways?\n",
        "> * Who might use this product that we didn't expect to use it, or for purposes we didn't initially\n",
        "> intend?\n",
        "\n",
        "See pp.119-120 for a bunch of good questions to put into your practice!\n",
        "\n",
        "> When everybody on a team has similar backgrounds, they are likely to have similar blind spots around ethical tasks.\n",
        "\n",
        "> ... first come up with a process, definition, set of questions etc., which is designed to resolve a problem. Then try to come up with an example in which the apparent solution results in a proposal that no one would consider acceptable. This can then lead to further refinement of the solution.\n",
        "\n",
        "Thinking about all these things may lead one to analysis paralysis or even worse, complete apathy.  We need to start with something and be okay with criticism and refactoring. Additionally, we need to be thoughtful in even spot on criticism of others' systems. I don't think most folks try to make something racist or mysoginistic or whatever, so instead of calling them a \"Hitler\" on Twitter when we taste something that looks to us like fasicism, maybe a phone call and one-on-one chat is the better and more productive move.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSwEOzDpHHBj"
      },
      "source": [
        "---\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. https://book.fast.ai - The book's website; it's updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc...\n",
        "\n",
        "2. https://forums.fast.ai/c/data-ethics/47 - Forum subcategory for all things \"data ethics\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2wEfmj5CLA8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}