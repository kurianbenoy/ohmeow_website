{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "how-to-learn-deep-learning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzKweyHaD/eWCPjbijy9u6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWJL4-Qwluy4"
      },
      "source": [
        "# \"What is machine learning\"\n",
        "> \"Insights from [\\\"Deep Learning for Coders with fastai & PyTorch\\\"](https://github.com/fastai/fastbook) and from around the world\"\n",
        "\n",
        "- toc: true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- hide_binder_badge: true\n",
        "- comments: true\n",
        "- author: Wayde Gilliam\n",
        "- categories: [fastai, fastbook, what-is, how-to]\n",
        "- image: images/articles/what-is-ai-ml-dl.jpg\n",
        "- hide: false\n",
        "- search_exclude: false\n",
        "- permalink: /what-is/machine-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nilyL8K728RC"
      },
      "source": [
        "Here we look at machine learning in general (of which deep learning is a subset) as well as the process of finetuning a pretrained ML model.  When you think of deep learning ... think neural networks.\n",
        "\n",
        "![](https://raw.githubusercontent.com/ohmeow/ohmeow_website/master/images/articles/what-is-ai-ml-dl.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfcRNESotGLU"
      },
      "source": [
        "## A picture\n",
        "\n",
        "![](https://github.com/ohmeow/ohmeow_website/blob/master/_notebooks/images/ajtfb-ch-1-deep_learning_overview.png?raw=1 \"Credit: Fastbook p.25\")\n",
        "\n",
        "## An explanation\n",
        "\n",
        "> \"Suppowe we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignemnt so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would 'learn' from its experince\" - Arthur Samuel {% fn 1 %}\n",
        "\n",
        "### Architecture vs. model\n",
        "> ... a ***model*** is a special kind of program: it's one that can do *many different things*, depending \n",
        "> on the **weights**. {% fn 2 %}\n",
        "\n",
        "> The functional form of the *model* is called its ***architecture***.\n",
        "\n",
        "> Important: The **architecture** is \"the *template* of the model that we're trying to fit; i.e., the actual mathematical function that we're passing the input data and parameters to\" ... whereas the **model** is a particular set of parameters + the architecture.\n",
        "\n",
        "### Parameters\n",
        "> **Weights** are just variables, and a **weight assignment** is a particuarl choice of values for those \n",
        "> variables. [Weights] are generally referred to as model ***parameters*** ... the term *weights* being\n",
        "> reserved for a particular type of model parameter. {% fn 3 %}\n",
        "\n",
        "> The *weights* are called ***parameters***.\n",
        "\n",
        "> Important: These parameters are the things that are \"learnt\"; the values that can be updated, whereas **activations** in a neural network are simply numbers as the result of some calculation.\n",
        "\n",
        "### Inputs vs.labels\n",
        "\n",
        "The ***inputs***, also known as your ***independent variable(s)*** [your `X`] is what your model uses to make ***predictions***. {% fn 4 %}\n",
        "\n",
        "The ***labels***, also known as your ***dependent variable(s)*** [your `y`] represent the correct target value for your task. {% fn 5 %}\n",
        "\n",
        "### Loss\n",
        "\n",
        "> The [model's] measure of performance is called the ***loss*** ... [the value of which depends on how well your model is able to predict] the correct ***labels***. {% fn 6 %}\n",
        "\n",
        "The ***loss*** is a measure of model performance that SGD can use to make your model better. A good loss function provides good gradients (slopes) that can be used to make even very minor changes to your weights so as to improve things. Visually, you want gentle rolling hills rather than abrupt steps or jagged peaks.\n",
        "\n",
        "> Important: You can think of the **loss** as the model's **metric**, that is, how it both understands how good it is and can help it improve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W70CEIUeo-cT"
      },
      "source": [
        "---\n",
        "\n",
        "## Transfer learning\n",
        "\n",
        "***Transfer learning*** is the process of taking a **\"pretrained model\"** that has been trained on a very large dataset with proven SOTA results, and **\"fine tuning\"** it for your specific task, which while likely similar to the task the pretrained model was trained for to one degree or another, is not the necesarily the same.\n",
        "\n",
        "### How does it work?\n",
        "1. The **head** of your model (the newly added part specific to your dataset/task) should be trained first since it is the only one with completely random weights. \n",
        "2. The degree to which your weights of the pretrained model will need to be updated is proportional to how similar your data is to the data it was trained on. The more dissimilar, the more the weights will need to be changed.\n",
        "3. Your model will only be as good as the data it was trained on, so make sure what you have is representative of what it will see in the real world. It \"can learn to operate on only the patterns seen in the input data used to train it.\"\n",
        "\n",
        "> The process of *training* (or *fitting*) the model is the process of finding a set of *parameter values* \n",
        "> (or *weights*) that specialize that general architecture into a model that works well for our \n",
        "> particular kind of data [and task]\n",
        "\n",
        "### What is the high-level approach in fastai?\n",
        "fastai provides a `fine_tune` method that uses proven tricks and hyperparameters for various DL tasks that the author's have found works well most of the time. {% fn 7 %}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUdQWU1s2hf_"
      },
      "source": [
        "---\n",
        "\n",
        "## What do we have at the end of training (or finetuning)?\n",
        "\n",
        "> ... once the model is trained - that is, once we've chosen our final weight assignments - then we can think of the weights as being *part of the model* since we're not varying them anymore. {% fn 8 %}\n",
        "\n",
        "This means a trained model can be treated like a typical function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKAQxMmyx8OB"
      },
      "source": [
        "---\n",
        "{{ '\"Chaper 1: Your Deep Learning Journey\". In *[The Fastbook](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)* p.21' | fndetail: 1 }}\n",
        "\n",
        "{{ 'Ibid.' | fndetail: 2 }}\n",
        "\n",
        "{{ 'Ibid., pp.21-22' | fndetail: 3 }}\n",
        "\n",
        "{{ 'Ibid., p.22' | fndetail: 4 }}\n",
        "\n",
        "{{ 'Ibid.' | fndetail: 5 }}\n",
        "\n",
        "{{ 'Ibid.' | fndetail: 6 }}\n",
        "\n",
        "{{ 'Ibid., pp.32-33. Includes a full discussion on how the method works' | fndetail: 7 }}\n",
        "\n",
        "{{ 'Ibid., p.22' | fndetail: 8 }}"
      ]
    }
  ]
}