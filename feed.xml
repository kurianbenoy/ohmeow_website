<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://ohmeow.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ohmeow.com/" rel="alternate" type="text/html" /><updated>2021-11-10T14:24:41-06:00</updated><id>https://ohmeow.com/feed.xml</id><title type="html">ohmeow</title><subtitle>A full-stack web application and ML development company.</subtitle><entry><title type="html">What are the differences between training, validation, and test datasets</title><link href="https://ohmeow.com/what-is/training-validation-test-sets" rel="alternate" type="text/html" title="What are the differences between training, validation, and test datasets" /><published>2021-11-09T00:00:00-06:00</published><updated>2021-11-09T00:00:00-06:00</updated><id>https://ohmeow.com/what-is/what-is-training-validation-test-sets</id><content type="html" xml:base="https://ohmeow.com/what-is/training-validation-test-sets">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/what-is-training-validation-test-sets.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here we look at the differences between training, validation, and test sets, and also both strategies and best practices for building each.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-is-a-training-set?&quot;&gt;What is a training set?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-training-set?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;A &lt;strong&gt;&lt;em&gt;training set&lt;/em&gt;&lt;/strong&gt; consits of the data your model sees during training. These are the inputs and labels your model will use to determine the loss and update it's parameters in a way that will hopefully lead to a model that works well for its given task.&lt;/p&gt;
&lt;h3 id=&quot;Why-do-we-need-a-training-set?&quot;&gt;Why do we need a training set?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-do-we-need-a-training-set?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Because a model needs something to train on. It should be representative of the data the model will see in the future, and it should be updated if/when you discover that is not the case.&lt;/p&gt;
&lt;h3 id=&quot;How-to-use-a-training-set?&quot;&gt;How to use a training set?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-use-a-training-set?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;To train a model on examples resembling that which the model will seen in the future. More is generally better, but quality is king (e.g., bad data in, bad data out).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To provide augmented examples for your model to see so as to increase the number of examples and better reflect what the model may see in the real world.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-is-a-validation-set?&quot;&gt;What is a validation set?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-validation-set?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;A &lt;strong&gt;&lt;em&gt;validation set&lt;/em&gt;&lt;/strong&gt; (also know as the &quot;development set&quot;) does not include any data from the &lt;strong&gt;&lt;em&gt;training set&lt;/em&gt;&lt;/strong&gt;.  It's purpose to is gauge the generalization prowess of your model and also ensure you are neight overfitting or underfitting.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;If [the model] makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by &lt;em&gt;actually having seen that particular item&lt;/em&gt;.&quot; &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;Why-do-we-need-a-validation-set?&quot;&gt;Why do we need a validation set?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-do-we-need-a-validation-set?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;&lt;p&gt;&quot;[because] what we care about is how well our model works on &lt;em&gt;previously unseen images&lt;/em&gt; ... the longer
you train for, the better your accuracy will get on the training set ... as the model starts to memorize
the training set rather than finding generalizable underlying patterns in the data = &lt;strong&gt;overfitting&lt;/strong&gt;&quot; &lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/fastai/fastbook/41a60e44d588139a03452f1907359fc2322f8d5f/images/att_00000.png&quot; alt=&quot;&quot; /&gt;
&lt;strong&gt;&lt;em&gt;Overfitting&lt;/em&gt;&lt;/strong&gt; happens when the model &quot;remembers specific features of the input data, rather than generalizing well to data not seen during training.&quot; &lt;sup id=&quot;fnref-3&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&lt;strong&gt;ALWAYS&lt;/strong&gt; overfit before anything else.  It is your training loss gets better while your validation loss gets worse ... in other words, if you&amp;#8217;re validation loss is improving, even if not to the extent of your training loss, you are &lt;em&gt;not&lt;/em&gt; overfitting
&lt;/div&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&lt;strong&gt;ALWAYS&lt;/strong&gt; include a validation set.
&lt;/div&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&lt;strong&gt;ALWAYS&lt;/strong&gt; use the validation set to measure your accuracy (or any metrics).
&lt;/div&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&lt;strong&gt;ALWAYS&lt;/strong&gt; set the &lt;code&gt;seed&lt;/code&gt; parameter so that you &quot;get the same validation set every time&quot; so that &quot;if we change our model and retrain it, we know any differences are due to the changes to the model, not due to having a different random validation set.&quot;
&lt;/div&gt;&lt;sup id=&quot;fnref-4&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;br /&gt;
&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;For a good discussion of how to achieve predictable randomness, see &lt;a href=&quot;https://forums.fast.ai/t/lesson1-reproducible-results-setting-seed-not-working/37921/5&quot;&gt;this discussion&lt;/a&gt; on the fast.ai forums. There are actually several seeds you need to set and in several places when using fast.ai to achieve reproducibility.
&lt;/div&gt;&lt;/p&gt;
&lt;h3 id=&quot;How-to-use-a-validation-set?&quot;&gt;How to use a validation set?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-use-a-validation-set?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;It gives us a sense of how well our model is doing on examples &lt;em&gt;it hasn't seen&lt;/em&gt;, which makes sense since the ultimate worth of a model is in how well it generalizes to things unseen in the future.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The validation set also informs us how we may change the  &lt;strong&gt;&lt;em&gt;hyperparamters&lt;/em&gt;&lt;/strong&gt; (e.g., model architecture, learning rates, data augmentation, etc...) to improve results.  These parameters are NOT learned ... they are choices WE make that affect the learning of the model parameters. &lt;sup id=&quot;fnref-5&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-5&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-is-a-test-set?&quot;&gt;What is a test set?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-test-set?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;A &lt;strong&gt;&lt;em&gt;test set&lt;/em&gt;&lt;/strong&gt; ensures that we aren't overfitting our hyperparameter choices; it is held back even from ourselves and used to evaulate the model at the very end.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;[Since we] are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values ... subsequent version of the model are, indirectly, shaped by us having seen the validation data ... [and therefore], we are in danger of overfitting the validation data through human trial and error and exploration.&quot; 
&lt;sup id=&quot;fnref-6&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-6&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Note:A key property of the validation and test sets is that they must be representative of the new data you will see in the future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;Why-do-we-need-a-test-set?&quot;&gt;Why do we need a test set?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-do-we-need-a-test-set?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;To ensure we aren't inadvertently causing the model to overfit via our hyperparameter tuning which happens as a result of us looking at the validation set. It is a completely hidden dataset; it isn't used for training or tuning, only for measuring performance.&lt;/p&gt;
&lt;h3 id=&quot;How-to-use-a-test-set?&quot;&gt;How to use a test set?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-use-a-test-set?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;If evaluating 3rd party solutions. You'll want to know how to create a good test set and how to create a good baseline model.  Hold these out from the potential consultants and use them to fairly evaluate their work.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To ensure you aren't overfitting your model as a result of validation set examination. As with the validation set, a good test set offers further assurance your model isn't learning particular ancillary features of particular things in your images.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;How-to-create-good-validation-and-test-sets&quot;&gt;How to create good validation and test sets&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-create-good-validation-and-test-sets&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;It isn't always as easy as randomly shuffling your data!&lt;/p&gt;
&lt;p&gt;Again, what both of these sets should haven in common is that they &quot;must be representative of the new data you will see in the future.&quot;  And what this looks like often dependes on your use case and task. 
&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;You really need to think about what you need to predict and what you&amp;#8217;d look at to make that prediction. You also need to make sure your training data is qualitatively different enough from your real world data (e.g., what the validation and test sets represent) as to learn patterns and not specific examples.
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, consider cases where historical data is required to predict the future, for example of quant traders use &quot;&lt;em&gt;backtesting&lt;/em&gt; to check whether their models are predictive of future periods, based on past data&quot; &lt;sup id=&quot;fnref-7&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-7&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&quot;For a &lt;strong&gt;time series&lt;/strong&gt; ... (where you are using historical data to build a model for use in the future ... you will want to choose a continuous section with the latest dates as your validation set&quot; 
&lt;/div&gt;&lt;sup id=&quot;fnref-8&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-8&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&quot;A &lt;strong&gt;second&lt;/strong&gt; common case occurs when you can easily anticipate ways the data you will be making predictions for in production may be &lt;em&gt;qualitatively different&lt;/em&gt; from the data you have to train your model with.&quot; &lt;sup id=&quot;fnref-9&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-9&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;As an example of this, &lt;a href=&quot;https://www.kaggle.com/c/state-farm-distracted-driver-detection&quot;&gt;the Kaggle distracted driver competition&lt;/a&gt; is used. In it, based on pictures of drivers you need to predict categories of distraction. Since the goal of such a model would be to make good predictions against &lt;strong&gt;&lt;em&gt;drivers the model hasn't seen&lt;/em&gt;&lt;/strong&gt;, it would make sense to create a validation and also a test set consiting of specific drivers the training set doesn't include (in fact, the competition's test set is exactly that!). &quot;If you used all the people in training your model, your model might be overfitting to the paricipants of those specific people and not just learning the states (texting, eating, etc.).&quot; &lt;sup id=&quot;fnref-10&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-10&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Another example of this is &lt;a href=&quot;https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring&quot;&gt;the Kaggle fisheries competition&lt;/a&gt; where the objective is to predict the species of fish caught on fishing boats. As the goal of such a model is to predict the species on other/future boats, it makes sense then that &quot;the test set consisted of images from boats that didn't appear in the training data, so in this case you'd want your validation set to also include boats that are not in the training set.&quot; &lt;sup id=&quot;fnref-11&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-11&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;
&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;Start with training a model and let the results guide your EDA!
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;For a stellar example of how this looks in practice, &lt;a href=&quot;https://twitter.com/borisdayma/status/1447939363296489473&quot;&gt;see this thread from Boris Dayma&lt;/a&gt; on an issue he noticed when looking at his results on the training and validation sets.  &lt;strong&gt;&lt;em&gt;Note how his EDA was directed via training a model&lt;/em&gt;&lt;/strong&gt; ... and also make sure to read through all the comments, replies, etc... for other things to pay attention too when seeing unusual results during training (there is a lot of good stuff there). Ultimately, in his case, what he found out was that the dataset was imbalanced and the imbalanced data was getting lumped together in the same batches due to poor shuffling strategy.  He documents &lt;a href=&quot;https://twitter.com/borisdayma/status/1448355381374242816&quot;&gt;his fix in a subsequent thread&lt;/a&gt; so check that out too.
&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;Knowing how to read your training/validation results drives EDA and will lead to better train/validation/test splits.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In &lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;The Fastbook&lt;/a&gt;&lt;/em&gt; p.49&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. Ibid., p.29&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. Ibid.&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-4&quot;&gt;4. Ibid&lt;a href=&quot;#fnref-4&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-5&quot;&gt;5. Ibid., p.49&lt;a href=&quot;#fnref-5&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-6&quot;&gt;6. Ibid.&lt;a href=&quot;#fnref-6&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-7&quot;&gt;7. Ibid., p.53&lt;a href=&quot;#fnref-7&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-8&quot;&gt;8. Ibid., p.51. There are some really good illustraions on pp.51 and 52 with some follow-up intutition on page 53 wrt to time series splits&lt;a href=&quot;#fnref-8&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-9&quot;&gt;9. Ibid., p.53&lt;a href=&quot;#fnref-9&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-10&quot;&gt;10. Ibid.&lt;a href=&quot;#fnref-10&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-11&quot;&gt;11. Ibid. pp.53-54&lt;a href=&quot;#fnref-11&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Wayde Gilliam</name></author><category term="fastai" /><category term="fastbook" /><category term="fastbook-chapter-1" /><category term="what-is" /><category term="how-to" /><category term="datasets" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ohmeow.com/images/articles/train-validation-test-sets.png" /><media:content medium="image" url="https://ohmeow.com/images/articles/train-validation-test-sets.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What is machine learning</title><link href="https://ohmeow.com/what-is/machine-learning" rel="alternate" type="text/html" title="What is machine learning" /><published>2021-11-09T00:00:00-06:00</published><updated>2021-11-09T00:00:00-06:00</updated><id>https://ohmeow.com/what-is/what-is-machine-learning</id><content type="html" xml:base="https://ohmeow.com/what-is/machine-learning">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/what-is-machine-learning.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here we look at machine learning in general (of which deep learning is a subset) as well as the process of finetuning a pretrained ML model.  When you think of deep learning ... think neural networks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ohmeow/ohmeow_website/master/images/articles/what-is-ai-ml-dl.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;A-picture&quot;&gt;A picture&lt;a class=&quot;anchor-link&quot; href=&quot;#A-picture&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://github.com/ohmeow/ohmeow_website/blob/master/_notebooks/images/ajtfb-ch-1-deep_learning_overview.png?raw=1&quot; alt=&quot;&quot; title=&quot;Credit: Fastbook p.25&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;An-explanation&quot;&gt;An explanation&lt;a class=&quot;anchor-link&quot; href=&quot;#An-explanation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;&quot;Suppowe we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignemnt so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would 'learn' from its experince&quot; - Arthur Samuel &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Architecture-vs.-model&quot;&gt;Architecture vs. model&lt;a class=&quot;anchor-link&quot; href=&quot;#Architecture-vs.-model&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;&lt;p&gt;... a &lt;strong&gt;&lt;em&gt;model&lt;/em&gt;&lt;/strong&gt; is a special kind of program:it's one that can do &lt;em&gt;many different things&lt;/em&gt;, depending &amp;gt; on the &lt;strong&gt;weights&lt;/strong&gt;. &lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The functional form of the &lt;em&gt;model&lt;/em&gt; is called its &lt;strong&gt;&lt;em&gt;architecture&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;The &lt;strong&gt;architecture&lt;/strong&gt; is &quot;the &lt;em&gt;template&lt;/em&gt; of the model that we&amp;#8217;re trying to fit; i.e., the actual mathematical function that we&amp;#8217;re passing the input data and parameters to&quot; ... whereas the &lt;strong&gt;model&lt;/strong&gt; is a particular set of parameters + the architecture.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Parameters&quot;&gt;Parameters&lt;a class=&quot;anchor-link&quot; href=&quot;#Parameters&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;Weights&lt;/strong&gt; are just variables, and a &lt;strong&gt;weight assignment&lt;/strong&gt; is a particuarl choice of values for those 
variables. [Weights] are generally referred to as model &lt;strong&gt;&lt;em&gt;parameters&lt;/em&gt;&lt;/strong&gt; ... the term &lt;em&gt;weights&lt;/em&gt; being
reserved for a particular type of model parameter. &lt;sup id=&quot;fnref-3&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;weights&lt;/em&gt; are called &lt;strong&gt;&lt;em&gt;parameters&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;These parameters are the things that are &quot;learnt&quot;; the values that can be updated, whereas &lt;strong&gt;activations&lt;/strong&gt; in a neural network are simply numbers as the result of some calculation.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Inputs-vs.labels&quot;&gt;Inputs vs.labels&lt;a class=&quot;anchor-link&quot; href=&quot;#Inputs-vs.labels&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The &lt;strong&gt;&lt;em&gt;inputs&lt;/em&gt;&lt;/strong&gt;, also known as your &lt;strong&gt;&lt;em&gt;independent variable(s)&lt;/em&gt;&lt;/strong&gt; [your &lt;code&gt;X&lt;/code&gt;] is what your model uses to make &lt;strong&gt;&lt;em&gt;predictions&lt;/em&gt;&lt;/strong&gt;. &lt;sup id=&quot;fnref-4&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;&lt;em&gt;labels&lt;/em&gt;&lt;/strong&gt;, also known as your &lt;strong&gt;&lt;em&gt;dependent variable(s)&lt;/em&gt;&lt;/strong&gt; [your &lt;code&gt;y&lt;/code&gt;] represent the correct target value for your task. &lt;sup id=&quot;fnref-5&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-5&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Loss&quot;&gt;Loss&lt;a class=&quot;anchor-link&quot; href=&quot;#Loss&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;&lt;p&gt;The [model's] measure of performance is called the &lt;strong&gt;&lt;em&gt;loss&lt;/em&gt;&lt;/strong&gt; ... [the value of which depends on how well your model is able to predict] the correct &lt;strong&gt;&lt;em&gt;labels&lt;/em&gt;&lt;/strong&gt;. &lt;sup id=&quot;fnref-6&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-6&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;strong&gt;&lt;em&gt;loss&lt;/em&gt;&lt;/strong&gt; is a measure of model performance that SGD can use to make your model better. A good loss function provides good gradients (slopes) that can be used to make even very minor changes to your weights so as to improve things. Visually, you want gentle rolling hills rather than abrupt steps or jagged peaks.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;You can think of the &lt;strong&gt;loss&lt;/strong&gt; as the model&amp;#8217;s &lt;strong&gt;metric&lt;/strong&gt;, that is, how it both understands how good it is and can help it improve.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Transfer-learning&quot;&gt;Transfer learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Transfer-learning&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Transfer learning&lt;/em&gt;&lt;/strong&gt; is the process of taking a &lt;strong&gt;&quot;pretrained model&quot;&lt;/strong&gt; that has been trained on a very large dataset with proven SOTA results, and &lt;strong&gt;&quot;fine tuning&quot;&lt;/strong&gt; it for your specific task, which while likely similar to the task the pretrained model was trained for to one degree or another, is not the necesarily the same.&lt;/p&gt;
&lt;h3 id=&quot;How-does-it-work?&quot;&gt;How does it work?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-does-it-work?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;The &lt;strong&gt;head&lt;/strong&gt; of your model (the newly added part specific to your dataset/task) should be trained first since it is the only one with completely random weights. &lt;/li&gt;
&lt;li&gt;The degree to which your weights of the pretrained model will need to be updated is proportional to how similar your data is to the data it was trained on. The more dissimilar, the more the weights will need to be changed.&lt;/li&gt;
&lt;li&gt;Your model will only be as good as the data it was trained on, so make sure what you have is representative of what it will see in the real world. It &quot;can learn to operate on only the patterns seen in the input data used to train it.&quot;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;&lt;p&gt;The process of &lt;em&gt;training&lt;/em&gt; (or &lt;em&gt;fitting&lt;/em&gt;) the model is the process of finding a set of &lt;em&gt;parameter values&lt;/em&gt; 
(or &lt;em&gt;weights&lt;/em&gt;) that specialize that general architecture into a model that works well for our 
particular kind of data [and task]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;What-is-the-high-level-approach-in-fastai?&quot;&gt;What is the high-level approach in fastai?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-the-high-level-approach-in-fastai?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;fastai provides a &lt;code&gt;fine_tune&lt;/code&gt; method that uses proven tricks and hyperparameters for various DL tasks that the author's have found works well most of the time. &lt;sup id=&quot;fnref-7&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-7&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-do-we-have-at-the-end-of-training-(or-finetuning)?&quot;&gt;What do we have at the end of training (or finetuning)?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-do-we-have-at-the-end-of-training-(or-finetuning)?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;... once the model is trained - that is, once we've chosen our final weight assignments - then we can think of the weights as being &lt;em&gt;part of the model&lt;/em&gt; since we're not varying them anymore. &lt;sup id=&quot;fnref-8&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-8&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This means a trained model can be treated like a typical function.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In &lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;The Fastbook&lt;/a&gt;&lt;/em&gt; p.21&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. Ibid.&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. Ibid., pp.21-22&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-4&quot;&gt;4. Ibid., p.22&lt;a href=&quot;#fnref-4&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-5&quot;&gt;5. Ibid.&lt;a href=&quot;#fnref-5&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-6&quot;&gt;6. Ibid.&lt;a href=&quot;#fnref-6&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-7&quot;&gt;7. Ibid., pp.32-33. Includes a full discussion on how the method works&lt;a href=&quot;#fnref-7&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-8&quot;&gt;8. Ibid., p.22&lt;a href=&quot;#fnref-8&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Wayde Gilliam</name></author><category term="fastai" /><category term="fastbook" /><category term="fastbook-chapter-1" /><category term="what-is" /><category term="how-to" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ohmeow.com/images/articles/what-is-ai-ml-dl.jpg" /><media:content medium="image" url="https://ohmeow.com/images/articles/what-is-ai-ml-dl.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What is the difference between categorial and continuous datatypes</title><link href="https://ohmeow.com/what-is/categorial-continuous-datatypes" rel="alternate" type="text/html" title="What is the difference between categorial and continuous datatypes" /><published>2021-11-09T00:00:00-06:00</published><updated>2021-11-09T00:00:00-06:00</updated><id>https://ohmeow.com/what-is/what-is-categorial-continuous-datatypes</id><content type="html" xml:base="https://ohmeow.com/what-is/categorial-continuous-datatypes">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/what-is-categorial-continuous-datatypes.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When it comes to both your inputs and targets, knowing whether they are categorial or continuous guides how you represent them, the loss function you use, and the metrics you choose in measuring performance.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-is-a-categorical-datatype?&quot;&gt;What is a categorical datatype?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-categorical-datatype?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Categorical&lt;/em&gt;&lt;/strong&gt; data &quot;contains values that are one of a discrete set of choice&quot; such as gender, occupation, day of week, etc... &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3 id=&quot;What-if-our-target-is-categorical?&quot;&gt;What if our &lt;strong&gt;target&lt;/strong&gt; is categorical?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-if-our-target-is-categorical?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;If your target/lables are categorical, then you have either a &lt;strong&gt;multi-classification classification&lt;/strong&gt; problem (e.g., you are trying to predict a single class) or a &lt;strong&gt;multi-label classification problem&lt;/strong&gt; (e.g., you are trying to predict whether your example belongs to zero or multiple classes).&lt;/p&gt;
&lt;h4 id=&quot;Multi-classification-tasks&quot;&gt;Multi-classification tasks&lt;a class=&quot;anchor-link&quot; href=&quot;#Multi-classification-tasks&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;For multi-classification tasks, a sensible loss function would be &lt;a href=&quot;https://ohmeow.com/posts/2020/04/04/understanding-cross-entropy-loss.html&quot;&gt;cross entropy loss&lt;/a&gt; (&lt;code&gt;nn.CrossEntropyLoss&lt;/code&gt;) and &lt;a href=&quot;https://ohmeow.com/what-is/a-metric#Metrics-to-use-based-on-task&quot;&gt;useful metrics&lt;/a&gt; are likely to include error rate, accuracy, F1, recall, and/or precision depending on your business objectices and the make up of your dataset.  For example, if you're dealing with a highly imbalanced dataset, choosing accuracy would lead to an inflated sense of model performance since it may be learning to just predict the most common class.
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;What if you need to predict &quot;None&quot;? This is more real world and covered nicely in Zach Mueller&amp;#8217;s &lt;a href=&quot;https://walkwithfastai.com/Unknown_Labels&quot;&gt;Recognizing Unknown Images (or the Unknown Label problem)&lt;/a&gt;.
&lt;/div&gt;&lt;/p&gt;
&lt;h4 id=&quot;Multi-label-tasks&quot;&gt;Multi-label tasks&lt;a class=&quot;anchor-link&quot; href=&quot;#Multi-label-tasks&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;For multi-label tasks, a sensible loss function would be binary cross entropy loss (BCE) (&lt;code&gt;nn.BCEWithLogitsLoss&lt;/code&gt;) and useful metrics are likely to include F1, recall, and/or precision depending on your business objectices and the make up of your dataset. Notice that I didn't include error rate, or its opposite accuracy, as their datasets are generally highly imbalanced.&lt;/p&gt;
&lt;h3 id=&quot;What-if-our-input-is-categorical?&quot;&gt;What if our &lt;strong&gt;input&lt;/strong&gt; is categorical?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-if-our-input-is-categorical?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Categorical inputs are generally represented by an &lt;strong&gt;embedding&lt;/strong&gt; (e.g., a vector of numbers). &lt;strong&gt;&lt;em&gt;Why?&lt;/em&gt;&lt;/strong&gt; Mostly because it gives your model the ability to provide a more complex representation of your category than a single numer would.&lt;/p&gt;
&lt;p&gt;For example, imagine that one of your inputs is day of week (e.g., Sunday, Monday, etc.) ... what does that mean? When combined with other inputs, its likely that the meaning of it is going to be much more nuanced than a single number can represent, and so we'd like to use multiple learned numbers.  This is what an embedding is.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-is-a-continuous-datatype?&quot;&gt;What is a continuous datatype?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-continuous-datatype?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Continuous&lt;/em&gt;&lt;/strong&gt; data is numerical that represents a quantity such as age, salary, prices, etc...&lt;/p&gt;
&lt;h3 id=&quot;What-if-our-target-is-continuous?&quot;&gt;What if our &lt;strong&gt;target&lt;/strong&gt; is continuous?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-if-our-target-is-continuous?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;If your target/labels are continuous, then you have a regression problem and the most likely loss function you would choose would be mean-square-error loss (MSE) (&lt;code&gt;nn.MSELoss&lt;/code&gt;)  and your metric MSE as well&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;... MSE is already a a useful metric for this task (although its' probably more interpretable after we take the square root)&quot; ... the &lt;strong&gt;RMSE&lt;/strong&gt; (% fn 3 %}&lt;/p&gt;
&lt;p&gt;Note:For tasks that predict a continuous number, consider using &lt;code&gt;y_range&lt;/code&gt; to constrain the network to predicting a value in the known range of valid values.&lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;What-if-our-input-is-continuous?&quot;&gt;What if our &lt;strong&gt;input&lt;/strong&gt; is continuous?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-if-our-input-is-continuous?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In many cases there isn't anything special you need to do, in others, it makes sense to scale these numbers so they are in the same range (usually 0 to 1) as the rest of your continuous inputs.  This process is called &lt;strong&gt;normalization&lt;/strong&gt;. &lt;sup id=&quot;fnref-4&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. The reason you would want to do this is so continuous values with bigger range of values (say 1000) don't drown out those with a smaller range (say 5) during model training.&lt;/p&gt;
&lt;h4 id=&quot;Normalization&quot;&gt;Normalization&lt;a class=&quot;anchor-link&quot; href=&quot;#Normalization&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&quot;When training a model, if helps if your input data is &lt;em&gt;normalizaed&lt;/em&gt; - that is, has a mean of 0 and a standard deviation of 1.
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;See &lt;a href=&quot;https://towardsdatascience.com/how-to-calculate-the-mean-and-standard-deviation-normalizing-datasets-in-pytorch-704bd7d05f4c&quot;&gt;How To Calculate the Mean and Standard Deviation — Normalizing Datasets in Pytorch&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Example 1&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Some raw values: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 1. calculate their mean and standard deviation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Their mean is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; and their standard deviation is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 2. normalize their values &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;normalized&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Here are their values after normalization: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalized&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Example 2&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Some raw values: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 1. calculate their mean and standard deviation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Their mean is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; and their standard deviation is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 2. normalize their values &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;normalized&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Here are their values after normalization: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalized&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Example 1
Some raw values: tensor([  0.,  50., 100.], dtype=torch.float64)
Their mean is 50.0 and their standard deviation is 50.0
Here are their values after normalization: tensor([-1.,  0.,  1.], dtype=torch.float64)

Example 2
Some raw values: tensor([    0.,  5000., 10000.], dtype=torch.float64)
Their mean is 5000.0 and their standard deviation is 5000.0
Here are their values after normalization: tensor([-1.,  0.,  1.], dtype=torch.float64)

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;fastai supplies a &lt;code&gt;Normalize&lt;/code&gt; transform you can use to do this ... &quot;it acts on a whole mini-batch at once, so you can add it to the &lt;code&gt;batch_tfms&lt;/code&gt; secion of your data block ... you need to pass to this transform the mean and standard deviation that you want to use. If you don't, &quot;fastai will automatically calculate them from a single batch of your data). p.241
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&quot;This means that when you distribute a model, you need to also distribute the statistics used for normalization.&quot; (p.242)
&lt;/div&gt;&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;&quot;... if you&amp;#8217;re using a model that someon else has trained, make sure you find out what normalization statistics they used an match them&quot; (p.242)
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In &lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;The Fastbook&lt;/a&gt;&lt;/em&gt; p.46&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. Ibid. p.236. A good examle of how RMSE provides a reasonable metric for regression tasks is included on this page in reference to KeyPoint detection (e.g., detecting a point/coordinate, an x and y)&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. Ibid., p.47&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-4&quot;&gt;4. Ibid., pp.241-42, 320 includes an extended discussion of the why, how, and where &quot;normalization&quot; is needed. &lt;a href=&quot;#fnref-4&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Wayde Gilliam</name></author><category term="fastai" /><category term="fastbook" /><category term="fastbook-chapter-1" /><category term="fastbook-chapter-6" /><category term="fastbook-chapter-7" /><category term="what-is" /><category term="how-to" /><category term="datatypes" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ohmeow.com/images/articles/continuous-categorial-datatypes.png" /><media:content medium="image" url="https://ohmeow.com/images/articles/continuous-categorial-datatypes.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What is a metric</title><link href="https://ohmeow.com/what-is/a-metric" rel="alternate" type="text/html" title="What is a metric" /><published>2021-11-09T00:00:00-06:00</published><updated>2021-11-09T00:00:00-06:00</updated><id>https://ohmeow.com/what-is/what-is-a-metric</id><content type="html" xml:base="https://ohmeow.com/what-is/a-metric">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/what-is-a-metric.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;A-definition&quot;&gt;A definition&lt;a class=&quot;anchor-link&quot; href=&quot;#A-definition&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Metrics&lt;/strong&gt; are a human-understandable measures of model quality whereas the &lt;strong&gt;loss&lt;/strong&gt; is the machine's.  They are based on your validation set and are what you really care about, whereas the loss is &quot;a measure of performance&quot; that the training system can use to update weights automatically.&lt;/p&gt;
&lt;p&gt;A good choice for loss is a function &quot;that is easy for &lt;strong&gt;&lt;em&gt;stochastic gradient descent (SGD)&lt;/em&gt;&lt;/strong&gt; to use, whereas a good choies for your metrics are functions that your business users will care about. Seldom are they the same because most metrics don't provide smooth gradients that SGD can use to update your model's weights.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;Again, they are based on your validation/test sets (not your training set). Ultimately, we want to have a model that generalizes well to inputs it was &lt;em&gt;not&lt;/em&gt; trained on, and this is what our validation/test sets represent. This is how we relay our model quality.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Examples&quot;&gt;Examples&lt;a class=&quot;anchor-link&quot; href=&quot;#Examples&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;There are a whole list of metrics built into the fastai library, &lt;a href=&quot;https://docs.fast.ai/metrics.html&quot;&gt;see here&lt;/a&gt;. Below I begin a listing of the most common ones as they come up in the fastbook (and from personal experience).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;error rate&lt;/strong&gt; = &quot;the proportion of images that were incorrectly identified.&quot; &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;accuracy&lt;/strong&gt; = the proportation of images that were correctly identified (&lt;code&gt;1 - error rate&lt;/code&gt;)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Metrics-to-use-based-on-task&quot;&gt;Metrics to use based on task&lt;a class=&quot;anchor-link&quot; href=&quot;#Metrics-to-use-based-on-task&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;style type=&quot;text/css&quot;&gt;
    #T_d9fb9ef0_3dba_11ec_8635_0242ac1c0002 th {
          border-style: solid;
          border-width: 1px;
    }    #T_d9fb9ef0_3dba_11ec_8635_0242ac1c0002 td {
          border-style: solid;
          border-width: 1px;
    }&lt;/style&gt;&lt;table id=&quot;T_d9fb9ef0_3dba_11ec_8635_0242ac1c0002&quot; class=&quot;dataframe&quot;&gt;&lt;thead&gt;    &lt;tr&gt;        &lt;th class=&quot;col_heading level0 col0&quot;&gt;Metric&lt;/th&gt;        &lt;th class=&quot;col_heading level0 col1&quot;&gt;Multiclass classification&lt;/th&gt;        &lt;th class=&quot;col_heading level0 col2&quot;&gt;Multilabel classification&lt;/th&gt;        &lt;th class=&quot;col_heading level0 col3&quot;&gt;Regression&lt;/th&gt;    &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
                &lt;tr&gt;
                                &lt;td id=&quot;T_d9fb9ef0_3dba_11ec_8635_0242ac1c0002row0_col0&quot; class=&quot;data row0 col0&quot;&gt;error rate&lt;/td&gt;
                        &lt;td id=&quot;T_d9fb9ef0_3dba_11ec_8635_0242ac1c0002row0_col1&quot; class=&quot;data row0 col1&quot;&gt;Yes&lt;/td&gt;
                        &lt;td id=&quot;T_d9fb9ef0_3dba_11ec_8635_0242ac1c0002row0_col2&quot; class=&quot;data row0 col2&quot;&gt;Yes*&lt;/td&gt;
                        &lt;td id=&quot;T_d9fb9ef0_3dba_11ec_8635_0242ac1c0002row0_col3&quot; class=&quot;data row0 col3&quot;&gt;No&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                                &lt;td id=&quot;T_d9fb9ef0_3dba_11ec_8635_0242ac1c0002row1_col0&quot; class=&quot;data row1 col0&quot;&gt;accuracy&lt;/td&gt;
                        &lt;td id=&quot;T_d9fb9ef0_3dba_11ec_8635_0242ac1c0002row1_col1&quot; class=&quot;data row1 col1&quot;&gt;Yes&lt;/td&gt;
                        &lt;td id=&quot;T_d9fb9ef0_3dba_11ec_8635_0242ac1c0002row1_col2&quot; class=&quot;data row1 col2&quot;&gt;Yes*&lt;/td&gt;
                        &lt;td id=&quot;T_d9fb9ef0_3dba_11ec_8635_0242ac1c0002row1_col3&quot; class=&quot;data row1 col3&quot;&gt;No&lt;/td&gt;
            &lt;/tr&gt;
    &lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;code&gt;*&lt;/code&gt; indicates that other metrics may be better for the given task.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In &lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;The Fastbook&lt;/a&gt;&lt;/em&gt; p.19&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Wayde Gilliam</name></author><category term="fastai" /><category term="fastbook" /><category term="fastbook-chapter-1" /><category term="what-is" /><category term="how-to" /><category term="metrics" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ohmeow.com/images/articles/tape-measure.jpg" /><media:content medium="image" url="https://ohmeow.com/images/articles/tape-measure.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What are ResNets &amp;amp; Why use it for computer vision tasks</title><link href="https://ohmeow.com/what-is/resnets" rel="alternate" type="text/html" title="What are ResNets &amp;amp; Why use it for computer vision tasks" /><published>2021-11-09T00:00:00-06:00</published><updated>2021-11-09T00:00:00-06:00</updated><id>https://ohmeow.com/what-is/what-are-resnets</id><content type="html" xml:base="https://ohmeow.com/what-is/resnets">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/what-are-resnets.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Arguably the best architecture for most computer vision tasks, here we take a look at &lt;strong&gt;ResNet&lt;/strong&gt; and how it can be used in fastai for a variety of such tasks.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-is-a-ResNet-&amp;amp;-Why-use-it-for-computer-vision-tasks?&quot;&gt;What is a ResNet &amp;amp; Why use it for computer vision tasks?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-ResNet-&amp;amp;-Why-use-it-for-computer-vision-tasks?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;A &lt;strong&gt;ResNet&lt;/strong&gt; is a model architecture that has proven to work well in CV tasks. Several variants exist with different numbers of layers with the larger architectures taking longer to train and more prone to overfitting especially with smaller datasets.&lt;/p&gt;
&lt;p&gt;The number represents the number of layers in this particular ResNet variant ... &quot;(other options are 18, 50, 101, and 152) ... model architectures with more layers take longer to train and are more prone to overfitting ... on the other hand, when using more data, they can be qite a bit more accurate.&quot; &lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3 id=&quot;What-other-things-can-use-images-recognizers-for-besides-image-tasks?&quot;&gt;What other things can use images recognizers for besides image tasks?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-other-things-can-use-images-recognizers-for-besides-image-tasks?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Sound, time series, malware classification ... &quot;a good rule of thumb for converting a dataset into an image representation: if the human eye can recognize categories from the images, then a deep learning model should be able to do so too.&quot; &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3 id=&quot;How-does-it-fare-against-more-recent-architectures-like-vision-transformers?&quot;&gt;How does it fare against more recent architectures like vision transformers?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-does-it-fare-against-more-recent-architectures-like-vision-transformers?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Pretty well apparently (at least at the time this post was written) ...

&lt;center&gt;
    &lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;I&amp;#39;m pleased to announce that the &amp;#39;ResNet strikes back&amp;#39; paper is now on arxiv! Moving the baseline forward to 80.4% top-1 for a vanilla ResNet-50 arch w/ better training recipes. No extra data, no distillation. &lt;a href=&quot;https://t.co/WP3UDXfV0r&quot;&gt;https://t.co/WP3UDXfV0r&lt;/a&gt;&lt;/p&gt;&amp;mdash; Ross Wightman (@wightmanr) &lt;a href=&quot;https://twitter.com/wightmanr/status/1444852719773122565?ref_src=twsrc%5Etfw&quot;&gt;October 4, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;ResNet-best-practices&quot;&gt;ResNet best practices&lt;a class=&quot;anchor-link&quot; href=&quot;#ResNet-best-practices&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;Start with a smaller ResNet (like 18 or 34) and move up as needed.
&lt;/div&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;If you have a lot of data, the bigger resnets will likely give you better results.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;An-example-using-the-high-level-API&quot;&gt;An example using the high-level API&lt;a class=&quot;anchor-link&quot; href=&quot;#An-example-using-the-high-level-API&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Step-1:-Build-our-DataLoaders&quot;&gt;Step 1: Build our DataLoaders&lt;a class=&quot;anchor-link&quot; href=&quot;#Step-1:-Build-our-DataLoaders&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastai.vision.all&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;untar_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;URLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PETS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;images&amp;#39;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;is_cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isupper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ImageDataLoaders&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_name_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_image_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_pct&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_func&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item_tfms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Why do we make images 224x224 pixels?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&quot;This is the standard size for historical reasons (old pretrained models require this size exactly) ... If you increase the size, you'll often get a model with better results since it will be able to focus on more details.&quot; &lt;sup id=&quot;fnref-3&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;
&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;Train on progressively larger image sizes using the weights trained on smaller sizes as a kind of pretrained model.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Step-2:-Build-our-cnn_learner&quot;&gt;Step 2: Build our &lt;code&gt;cnn_learner&lt;/code&gt;&lt;a class=&quot;anchor-link&quot; href=&quot;#Step-2:-Build-our-cnn_learner&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cnn_learner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;As you can see above, the architecture being used is a resnet with 18 layers.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Step-3:-Train&quot;&gt;Step 3: Train&lt;a class=&quot;anchor-link&quot; href=&quot;#Step-3:-Train&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fine_tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea &quot;&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;/th&gt;
      &lt;th&gt;train_loss&lt;/th&gt;
      &lt;th&gt;valid_loss&lt;/th&gt;
      &lt;th&gt;error_rate&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.161614&lt;/td&gt;
      &lt;td&gt;0.040670&lt;/td&gt;
      &lt;td&gt;0.013532&lt;/td&gt;
      &lt;td&gt;01:03&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea &quot;&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;/th&gt;
      &lt;th&gt;train_loss&lt;/th&gt;
      &lt;th&gt;valid_loss&lt;/th&gt;
      &lt;th&gt;error_rate&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.062475&lt;/td&gt;
      &lt;td&gt;0.020072&lt;/td&gt;
      &lt;td&gt;0.006766&lt;/td&gt;
      &lt;td&gt;01:04&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For more information on how transfer learning works, and the &lt;code&gt;fine_tune&lt;/code&gt; method in particuarl, see this section in my &lt;a href=&quot;https://ohmeow.com/what-is/machine-learning#Transfer-learning&quot;&gt;&quot;What is machine learning&quot; post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For more metrics like &lt;code&gt;error_rate&lt;/code&gt;, see my &lt;a href=&quot;https://ohmeow.com/what-is/a-metric&quot;&gt;&quot;What is a metric&quot; post&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. &quot;Chaper 1: Your Deep Learning Journey&quot;. In &lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;The Fastbook&lt;/a&gt;&lt;/em&gt; pp.30-31.&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. Ibid., p.39. Pages 36-39 provides several examples of how non-image data can be converted to an image for such a purpose.&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. Ibid., p.28&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/vnd.jupyter.widget-state+json&quot;&gt;
{&quot;55007a0aa5074700be062c2347e64fb3&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;HBoxModel&quot;, &quot;model_module_version&quot;: &quot;1.5.0&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;HBoxView&quot;, &quot;_dom_classes&quot;: [], &quot;_model_name&quot;: &quot;HBoxModel&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;box_style&quot;: &quot;&quot;, &quot;layout&quot;: &quot;IPY_MODEL_eac7de645cb24820a264714b152a3add&quot;, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;children&quot;: [&quot;IPY_MODEL_f8e8797b82b24e9797957a2b92d8056a&quot;, &quot;IPY_MODEL_2086a8ad3a834a87ae0de85f979848fc&quot;, &quot;IPY_MODEL_d3992b1f8b2b4934a58d123ef9dc1610&quot;]}}, &quot;eac7de645cb24820a264714b152a3add&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;model_module_version&quot;: &quot;1.2.0&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}, &quot;f8e8797b82b24e9797957a2b92d8056a&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;HTMLModel&quot;, &quot;model_module_version&quot;: &quot;1.5.0&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;HTMLView&quot;, &quot;style&quot;: &quot;IPY_MODEL_d5d61524d8884dfa922467ab506579de&quot;, &quot;_dom_classes&quot;: [], &quot;description&quot;: &quot;&quot;, &quot;_model_name&quot;: &quot;HTMLModel&quot;, &quot;placeholder&quot;: &quot;\u200b&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;value&quot;: &quot;100%&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;description_tooltip&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;layout&quot;: &quot;IPY_MODEL_dc803f68c4194b63a46718daac142d29&quot;}}, &quot;2086a8ad3a834a87ae0de85f979848fc&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;FloatProgressModel&quot;, &quot;model_module_version&quot;: &quot;1.5.0&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;ProgressView&quot;, &quot;style&quot;: &quot;IPY_MODEL_9bdb240d8a714c7dbc394dcecb01ded2&quot;, &quot;_dom_classes&quot;: [], &quot;description&quot;: &quot;&quot;, &quot;_model_name&quot;: &quot;FloatProgressModel&quot;, &quot;bar_style&quot;: &quot;success&quot;, &quot;max&quot;: 46830571, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;value&quot;: 46830571, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;orientation&quot;: &quot;horizontal&quot;, &quot;min&quot;: 0, &quot;description_tooltip&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;layout&quot;: &quot;IPY_MODEL_7c4cb3e1519d4de4a63e78119ceeede0&quot;}}, &quot;d3992b1f8b2b4934a58d123ef9dc1610&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;HTMLModel&quot;, &quot;model_module_version&quot;: &quot;1.5.0&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;HTMLView&quot;, &quot;style&quot;: &quot;IPY_MODEL_15a5bd961fff498fa8516de9fdcc0ca7&quot;, &quot;_dom_classes&quot;: [], &quot;description&quot;: &quot;&quot;, &quot;_model_name&quot;: &quot;HTMLModel&quot;, &quot;placeholder&quot;: &quot;\u200b&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;value&quot;: &quot; 44.7M/44.7M [00:00&amp;lt;00:00, 97.4MB/s]&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;description_tooltip&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;layout&quot;: &quot;IPY_MODEL_605d70a5cef44be9a1944ab5a26a5051&quot;}}, &quot;d5d61524d8884dfa922467ab506579de&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;DescriptionStyleModel&quot;, &quot;model_module_version&quot;: &quot;1.5.0&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;StyleView&quot;, &quot;_model_name&quot;: &quot;DescriptionStyleModel&quot;, &quot;description_width&quot;: &quot;&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;}}, &quot;dc803f68c4194b63a46718daac142d29&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;model_module_version&quot;: &quot;1.2.0&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}, &quot;9bdb240d8a714c7dbc394dcecb01ded2&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;ProgressStyleModel&quot;, &quot;model_module_version&quot;: &quot;1.5.0&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;StyleView&quot;, &quot;_model_name&quot;: &quot;ProgressStyleModel&quot;, &quot;description_width&quot;: &quot;&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;bar_color&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;}}, &quot;7c4cb3e1519d4de4a63e78119ceeede0&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;model_module_version&quot;: &quot;1.2.0&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}, &quot;15a5bd961fff498fa8516de9fdcc0ca7&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;DescriptionStyleModel&quot;, &quot;model_module_version&quot;: &quot;1.5.0&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;StyleView&quot;, &quot;_model_name&quot;: &quot;DescriptionStyleModel&quot;, &quot;description_width&quot;: &quot;&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;}}, &quot;605d70a5cef44be9a1944ab5a26a5051&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;model_module_version&quot;: &quot;1.2.0&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}}
&lt;/script&gt;</content><author><name>Wayde Gilliam</name></author><category term="fastai" /><category term="fastbook" /><category term="fastbook-chapter-1" /><category term="what-is" /><category term="computer vision" /><category term="resnet" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ohmeow.com/images/articles/resnet.png" /><media:content medium="image" url="https://ohmeow.com/images/articles/resnet.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How to learn (deep learning)</title><link href="https://ohmeow.com/how-to/learn-deep-learning" rel="alternate" type="text/html" title="How to learn (deep learning)" /><published>2021-11-09T00:00:00-06:00</published><updated>2021-11-09T00:00:00-06:00</updated><id>https://ohmeow.com/how-to/how-to-learn-deep-learning</id><content type="html" xml:base="https://ohmeow.com/how-to/learn-deep-learning">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/how-to-learn-deep-learning.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Cervantes once wrote that &quot;the journey is better than the inn&quot;, but I rather like to think that the journey &lt;em&gt;is&lt;/em&gt; the inn.&lt;/p&gt;
&lt;p&gt;It means that irrespective to its difficulties (and likely because of them), your adventures is what you look back on with fondness at its end rather than the end itself.  It's why I enjoy reading &quot;The Lord of the Rings&quot; every five years or so, where as I age and experience the hand life has dealt me, I find myself appreciating different aspects of the story and gaining new insights into what I value and into what I want to be as a human being. I find my journey with deep learning to be roughly analgous to that.&lt;/p&gt;
&lt;p&gt;I've been a part of &lt;a href=&quot;https://forums.fast.ai/u/wgpubs/summary&quot;&gt;the fast.ai community&lt;/a&gt; for several years. I've been through &lt;a href=&quot;https://course.fast.ai/&quot;&gt;the course&lt;/a&gt; multiple times (since it was using theano back in the old days), I've contributed to the library, and use it as the basis for one of &lt;a href=&quot;https://ohmeow.github.io/blurr/&quot;&gt;my own&lt;/a&gt;.  And as with each course, a re-reading of the book brings new insights, ideas, and revelations.&lt;/p&gt;
&lt;p&gt;And so here I begin with my meandering thoughts and reflections from yet another reading of what I consider &quot;The Lord of the Rings&quot; of deep learning. As such, it makes sense to being with the most foundational topic in fastbook, &lt;strong&gt;&quot;How do you learn Deep Learning?&quot;&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;You-can-do-this!&quot;&gt;You can do this!&lt;a class=&quot;anchor-link&quot; href=&quot;#You-can-do-this!&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Hi, everybody; I'm Jeremy ... I do not have any formal technical education ... didn't have great grades.
I was much more interested in doing real projects. &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is meaningful to me as someone with a BA in History and a MA in Theology. It's a reminder that if you want something, it's within your grasp to make it happen if you are willing to put in the work. It's also a reminder that key to getting there is actually doing something!  If find too many people thinking that if they just get into that school, or if they can just take that class, then they'll be a good software enginner or deep learning practitioner.  The reality is that the &lt;em&gt;only&lt;/em&gt; way you get there is by doing it ... just like pull-ups (which aren't much fun when you're starting out and/or you're 49 and overweight).&lt;/p&gt;
&lt;h2 id=&quot;The-problem-with-traditional-education&quot;&gt;The problem with traditional education&lt;a class=&quot;anchor-link&quot; href=&quot;#The-problem-with-traditional-education&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;... how math is taught - we require students to spend years doing rote memorization and learning dry
disconnected &lt;em&gt;fundatmentals&lt;/em&gt; that we claim will pay off later, long after most of them quit the subject. &lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This also is the problem with higher education in general, where young people spend at least four to five years learning things they already learned in High School or else things they don't really care about and will be forgotten right after finals, spending in excess of $100,000 for the privilege of it and likely going into debt in the tens of thousands of dollars, all with this idea that having done it they will be prepared for the real world.  Unfortunately, that's not how it works. Whether you are in a university of even go to university, what matter is what you do ... not what classes you took or what your GPA is.&lt;/p&gt;
&lt;h2 id=&quot;Deep-Learning-(and-coding-in-general)-is-an-art-maybe-more-so-than-a-science&quot;&gt;Deep Learning (and coding in general) is an art maybe more so than a science&lt;a class=&quot;anchor-link&quot; href=&quot;#Deep-Learning-(and-coding-in-general)-is-an-art-maybe-more-so-than-a-science&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;The hardest part of deep learning is artisanal. &lt;sup id=&quot;fnref-3&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I remember going to an iOS conference way back in the day and a conference speaker asking how many folks in the session I was sitting in had a background in music. 80-90% of the audience raised their hands.  Sure, there is math and stats and a science to deep learning, but like any coding enterprise, it's an art ... with some artists being better than others along with room for improvement regardless of whether you're Van Gough or painting by the numbers.&lt;/p&gt;
&lt;h2 id=&quot;Doing-is-how-you-learn,-and-what-you've-done-is-what-matters&quot;&gt;Doing is how you learn, and what you've done is what matters&lt;a class=&quot;anchor-link&quot; href=&quot;#Doing-is-how-you-learn,-and-what-you've-done-is-what-matters&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;... focus on your hobbies and passions ... Common character traits in the people who do well at deep learning include playfulness and curiosity. &lt;sup id=&quot;fnref-4&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;at Tesla .. CEO Elon Musk says 'A PhD is definitely not required. All that matters is a deep understanding of AI &amp;amp; ability to implement NNs in a way that is actually useful .... Don't care if you even graduated High School.' &lt;sup id=&quot;fnref-5&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-5&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;... the most important thing for learning deep learning is writing code and experimenting.&quot; &lt;sup id=&quot;fnref-6&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-6&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Folks-to-follow&quot;&gt;Folks to follow&lt;a class=&quot;anchor-link&quot; href=&quot;#Folks-to-follow&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;It's always helpful to have some role models; folks who practice the lessons learned above and can help you along your journey.&lt;/p&gt;
&lt;p&gt;For starters, consider this image of the top 12 users based on most likes in the fast.ai forums:
&lt;img src=&quot;https://github.com/ohmeow/ohmeow_website/blob/master/images/articles/20211102-fastai-forums-top-12.png?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Aside from the founders of &lt;a href=&quot;https://www.fast.ai/&quot;&gt;fast.ai&lt;/a&gt; and a bunch of them working for noteable ML companies like &lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt; and &lt;a href=&quot;https://wandb.ai/site&quot;&gt;Weights &amp;amp; Biases&lt;/a&gt;, I can think of at least &lt;strong&gt;&lt;em&gt;FOUR&lt;/em&gt;&lt;/strong&gt; things these folks have in common:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;They are &lt;strong&gt;fearless in asking what they may have even considered, dumb questions&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;They are &lt;strong&gt;active in researching the answers to their own questions&lt;/strong&gt; (even the dumb ones) and those asked by others.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;They are &lt;strong&gt;active in teaching&lt;/strong&gt; others through blogs, books, open source libraries, study groups, and podcasts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;They build&lt;/strong&gt; things! That is, they all have experience building models and making them usable via deployed applications and/or in kaggle compeititions.  Anyone can bake a half-cooked model in a Jupyter notebook, but few can turn it into something others can use.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These traits aren't just key to learning deep learning; they are key to learning anything!  Practice them and you guarantee yourself success in learning anything you've set your mind on.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-if-I-can-only-follow-three?&quot;&gt;What if I can only follow three?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-if-I-can-only-follow-three?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Aside from Jeremy (&lt;a href=&quot;https://twitter.com/jeremyphoward&quot;&gt;@jeremyphoward&lt;/a&gt;), who's a given, if I could only follow three people who have mastered to art of learning deep learning, they would be ...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Radek Osmulsk&lt;/strong&gt;: (twitter: &lt;a href=&quot;https://twitter.com/radekosmulski&quot;&gt;@radekosmulski&lt;/a&gt;)

&lt;center&gt;
    &lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;If you found this of value, you might be interested in a book on learning deep learning that I wrote&lt;br /&gt;&lt;br /&gt;check it out here &amp;gt;&amp;gt;&amp;gt; &lt;a href=&quot;https://t.co/ApKlm8BRmy&quot;&gt;https://t.co/ApKlm8BRmy&lt;/a&gt;&lt;/p&gt;&amp;mdash; Radek Osmulski (@radekosmulski) &lt;a href=&quot;https://twitter.com/radekosmulski/status/1455527697661169664?ref_src=twsrc%5Etfw&quot;&gt;November 2, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zach Mueller&lt;/strong&gt;: (twitter: &lt;a href=&quot;https://twitter.com/TheZachMueller&quot;&gt;@TheZachMueller&lt;/a&gt;)

&lt;center&gt;
    &lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;To me, I think it boiled down to how I learned. I took those two courses essentially over the course of a year or so. Approaching each lesson slowly, and letting myself wander in the related concepts, learning as much as I could through online communities.&lt;/p&gt;&amp;mdash; Zach Mueller (@TheZachMueller) &lt;a href=&quot;https://twitter.com/TheZachMueller/status/1451941577841127433?ref_src=twsrc%5Etfw&quot;&gt;October 23, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sanyam Bhutani&lt;/strong&gt;: (twitter: &lt;a href=&quot;https://twitter.com/bhutanisanyam1&quot;&gt;@bhutanisanyam1&lt;/a&gt;)

&lt;center&gt;
    &lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;The &lt;a href=&quot;https://twitter.com/PyTorch?ref_src=twsrc%5Etfw&quot;&gt;@PyTorch&lt;/a&gt; book reading group &lt;a href=&quot;https://twitter.com/weights_biases?ref_src=twsrc%5Etfw&quot;&gt;@weights_biases&lt;/a&gt; comes to an end🙏&lt;br /&gt;&lt;br /&gt;We had an incredible 10 weeks of learning!&lt;br /&gt;&lt;br /&gt;As a group wanted to extend our gratitude to the incredible authors: Eli, &lt;a href=&quot;https://twitter.com/lantiga?ref_src=twsrc%5Etfw&quot;&gt;@lantiga&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://twitter.com/ThomasViehmann?ref_src=twsrc%5Etfw&quot;&gt;@ThomasViehmann&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;A few words from our community:&lt;a href=&quot;https://t.co/3ODz6J1vad&quot;&gt;https://t.co/3ODz6J1vad&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sanyam Bhutani (@bhutanisanyam1) &lt;a href=&quot;https://twitter.com/bhutanisanyam1/status/1452599997493481472?ref_src=twsrc%5Etfw&quot;&gt;October 25, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;p&gt;Personally, I &lt;strong&gt;do&lt;/strong&gt; follow each of these individuals on twitter and you should too! Though I've never met any of them IRL, I consider the colleagues, friends, and amongst the most helpful for those looking to get started in machine learning.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;Twitter is imo the best place to network with fellow ML/DL practioners and stay up-to-date with the latest developments in ML in general
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-if-I'm-too-lazy-to-read-any-of-that-stuff-above?&quot;&gt;What if I'm too lazy to read any of that stuff above?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-if-I'm-too-lazy-to-read-any-of-that-stuff-above?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Well, you can watch this video (and then go back and read that stuff anyways) ...

&lt;center class=&quot;youtube-iframe-wrapper&quot;&gt;
    &lt;iframe width=&quot;730&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/_QUEXsHfsA0?t=935&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In &lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;The Fastbook&lt;/a&gt;&lt;/em&gt; p.8&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. Ibid., p.9&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. Ibid., p.10&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-4&quot;&gt;4. Ibid., p.11&lt;a href=&quot;#fnref-4&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-5&quot;&gt;5. Ibid.&lt;a href=&quot;#fnref-5&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-6&quot;&gt;6. Ibid.&lt;a href=&quot;#fnref-6&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Wayde Gilliam</name></author><category term="fastai" /><category term="fastbook" /><category term="fastbook-chapter-1" /><category term="how-to" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ohmeow.com/images/articles/thumbs_up_kid.jpeg" /><media:content medium="image" url="https://ohmeow.com/images/articles/thumbs_up_kid.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What are the differences between training, validation, and test datasets</title><link href="https://ohmeow.com/what-is/training-validation-test-sets" rel="alternate" type="text/html" title="What are the differences between training, validation, and test datasets" /><published>2021-11-05T00:00:00-05:00</published><updated>2021-11-05T00:00:00-05:00</updated><id>https://ohmeow.com/what-is/what-is-training-validation-test-sets</id><content type="html" xml:base="https://ohmeow.com/what-is/training-validation-test-sets">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-11-05-what-is-training-validation-test-sets.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here we look at the differences between training, validation, and test sets, and also both strategies and best practices for building each.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-is-a-training-set?&quot;&gt;What is a training set?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-training-set?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;A &lt;strong&gt;&lt;em&gt;training set&lt;/em&gt;&lt;/strong&gt; consits of the data your model sees during training. These are the inputs and labels your model will use to determine the loss and update it's parameters in a way that will hopefully lead to a model that works well for its given task.&lt;/p&gt;
&lt;h3 id=&quot;Why-do-we-need-a-training-set?&quot;&gt;Why do we need a training set?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-do-we-need-a-training-set?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Because a model needs something to train on. It should be representative of the data the model will see in the future, and it should be updated if/when you discover that is not the case.&lt;/p&gt;
&lt;h3 id=&quot;How-to-use-a-training-set?&quot;&gt;How to use a training set?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-use-a-training-set?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;To train a model on examples resembling that which the model will seen in the future. More is generally better, but quality is king (e.g., bad data in, bad data out).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To provide augmented examples for your model to see so as to increase the number of examples and better reflect what the model may see in the real world.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-is-a-validation-set?&quot;&gt;What is a validation set?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-validation-set?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;A &lt;strong&gt;&lt;em&gt;validation set&lt;/em&gt;&lt;/strong&gt; (also know as the &quot;development set&quot;) does not include any data from the &lt;strong&gt;&lt;em&gt;training set&lt;/em&gt;&lt;/strong&gt;.  It's purpose to is gauge the generalization prowess of your model and also ensure you are neight overfitting or underfitting.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;If [the model] makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by &lt;em&gt;actually having seen that particular item&lt;/em&gt;.&quot; &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;Why-do-we-need-a-validation-set?&quot;&gt;Why do we need a validation set?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-do-we-need-a-validation-set?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;&lt;p&gt;&quot;[because] what we care about is how well our model works on &lt;em&gt;previously unseen images&lt;/em&gt; ... the longer
you train for, the better your accuracy will get on the training set ... as the model starts to memorize
the training set rather than finding generalizable underlying patterns in the data = &lt;strong&gt;overfitting&lt;/strong&gt;&quot; &lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/fastai/fastbook/41a60e44d588139a03452f1907359fc2322f8d5f/images/att_00000.png&quot; alt=&quot;&quot; /&gt;
&lt;strong&gt;&lt;em&gt;Overfitting&lt;/em&gt;&lt;/strong&gt; happens when the model &quot;remembers specific features of the input data, rather than generalizing well to data not seen during training.&quot; &lt;sup id=&quot;fnref-3&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&lt;strong&gt;ALWAYS&lt;/strong&gt; overfit before anything else.  It is your training loss gets better while your validation loss gets worse ... in other words, if you&amp;#8217;re validation loss is improving, even if not to the extent of your training loss, you are &lt;em&gt;not&lt;/em&gt; overfitting
&lt;/div&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&lt;strong&gt;ALWAYS&lt;/strong&gt; include a validation set.
&lt;/div&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&lt;strong&gt;ALWAYS&lt;/strong&gt; use the validation set to measure your accuracy (or any metrics).
&lt;/div&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&lt;strong&gt;ALWAYS&lt;/strong&gt; set the &lt;code&gt;seed&lt;/code&gt; parameter so that you &quot;get the same validation set every time&quot; so that &quot;if we change our model and retrain it, we know any differences are due to the changes to the model, not due to having a different random validation set.&quot;
&lt;/div&gt;&lt;sup id=&quot;fnref-4&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;br /&gt;
&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;For a good discussion of how to achieve predictable randomness, see &lt;a href=&quot;https://forums.fast.ai/t/lesson1-reproducible-results-setting-seed-not-working/37921/5&quot;&gt;this discussion&lt;/a&gt; on the fast.ai forums. There are actually several seeds you need to set and in several places when using fast.ai to achieve reproducibility.
&lt;/div&gt;&lt;/p&gt;
&lt;h3 id=&quot;How-to-use-a-validation-set?&quot;&gt;How to use a validation set?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-use-a-validation-set?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;It gives us a sense of how well our model is doing on examples &lt;em&gt;it hasn't seen&lt;/em&gt;, which makes sense since the ultimate worth of a model is in how well it generalizes to things unseen in the future.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The validation set also informs us how we may change the  &lt;strong&gt;&lt;em&gt;hyperparamters&lt;/em&gt;&lt;/strong&gt; (e.g., model architecture, learning rates, data augmentation, etc...) to improve results.  These parameters are NOT learned ... they are choices WE make that affect the learning of the model parameters. &lt;sup id=&quot;fnref-5&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-5&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-is-a-test-set?&quot;&gt;What is a test set?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-test-set?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;A &lt;strong&gt;&lt;em&gt;test set&lt;/em&gt;&lt;/strong&gt; ensures that we aren't overfitting our hyperparameter choices; it is held back even from ourselves and used to evaulate the model at the very end.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;[Since we] are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values ... subsequent version of the model are, indirectly, shaped by us having seen the validation data ... [and therefore], we are in danger of overfitting the validation data through human trial and error and exploration.&quot; 
&lt;sup id=&quot;fnref-6&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-6&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Note:A key property of the validation and test sets is that they must be representative of the new data you will see in the future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;Why-do-we-need-a-test-set?&quot;&gt;Why do we need a test set?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-do-we-need-a-test-set?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;To ensure we aren't inadvertently causing the model to overfit via our hyperparameter tuning which happens as a result of us looking at the validation set. It is a completely hidden dataset; it isn't used for training or tuning, only for measuring performance.&lt;/p&gt;
&lt;h3 id=&quot;How-to-use-a-test-set?&quot;&gt;How to use a test set?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-use-a-test-set?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;If evaluating 3rd party solutions. You'll want to know how to create a good test set and how to create a good baseline model.  Hold these out from the potential consultants and use them to fairly evaluate their work.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To ensure you aren't overfitting your model as a result of validation set examination. As with the validation set, a good test set offers further assurance your model isn't learning particular ancillary features of particular things in your images.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;How-to-create-good-validation-and-test-sets&quot;&gt;How to create good validation and test sets&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-create-good-validation-and-test-sets&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;It isn't always as easy as randomly shuffling your data!&lt;/p&gt;
&lt;p&gt;Again, what both of these sets should haven in common is that they &quot;must be representative of the new data you will see in the future.&quot;  And what this looks like often dependes on your use case and task. 
&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;You really need to think about what you need to predict and what you&amp;#8217;d look at to make that prediction. You also need to make sure your training data is qualitatively different enough from your real world data (e.g., what the validation and test sets represent) as to learn patterns and not specific examples.
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, consider cases where historical data is required to predict the future, for example of quant traders use &quot;&lt;em&gt;backtesting&lt;/em&gt; to check whether their models are predictive of future periods, based on past data&quot; &lt;sup id=&quot;fnref-7&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-7&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&quot;For a &lt;strong&gt;time series&lt;/strong&gt; ... (where you are using historical data to build a model for use in the future ... you will want to choose a continuous section with the latest dates as your validation set&quot; 
&lt;/div&gt;&lt;sup id=&quot;fnref-8&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-8&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&quot;A &lt;strong&gt;second&lt;/strong&gt; common case occurs when you can easily anticipate ways the data you will be making predictions for in production may be &lt;em&gt;qualitatively different&lt;/em&gt; from the data you have to train your model with.&quot; &lt;sup id=&quot;fnref-9&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-9&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;As an example of this, &lt;a href=&quot;https://www.kaggle.com/c/state-farm-distracted-driver-detection&quot;&gt;the Kaggle distracted driver competition&lt;/a&gt; is used. In it, based on pictures of drivers you need to predict categories of distraction. Since the goal of such a model would be to make good predictions against &lt;strong&gt;&lt;em&gt;drivers the model hasn't seen&lt;/em&gt;&lt;/strong&gt;, it would make sense to create a validation and also a test set consiting of specific drivers the training set doesn't include (in fact, the competition's test set is exactly that!). &quot;If you used all the people in training your model, your model might be overfitting to the paricipants of those specific people and not just learning the states (texting, eating, etc.).&quot; &lt;sup id=&quot;fnref-10&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-10&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Another example of this is &lt;a href=&quot;https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring&quot;&gt;the Kaggle fisheries competition&lt;/a&gt; where the objective is to predict the species of fish caught on fishing boats. As the goal of such a model is to predict the species on other/future boats, it makes sense then that &quot;the test set consisted of images from boats that didn't appear in the training data, so in this case you'd want your validation set to also include boats that are not in the training set.&quot; &lt;sup id=&quot;fnref-11&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-11&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;
&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;Start with training a model and let the results guide your EDA!
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;For a stellar example of how this looks in practice, &lt;a href=&quot;https://twitter.com/borisdayma/status/1447939363296489473&quot;&gt;see this thread from Boris Dayma&lt;/a&gt; on an issue he noticed when looking at his results on the training and validation sets.  &lt;strong&gt;&lt;em&gt;Note how his EDA was directed via training a model&lt;/em&gt;&lt;/strong&gt; ... and also make sure to read through all the comments, replies, etc... for other things to pay attention too when seeing unusual results during training (there is a lot of good stuff there). Ultimately, in his case, what he found out was that the dataset was imbalanced and the imbalanced data was getting lumped together in the same batches due to poor shuffling strategy.  He documents &lt;a href=&quot;https://twitter.com/borisdayma/status/1448355381374242816&quot;&gt;his fix in a subsequent thread&lt;/a&gt; so check that out too.
&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;Knowing how to read your training/validation results drives EDA and will lead to better train/validation/test splits.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In &lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;The Fastbook&lt;/a&gt;&lt;/em&gt; p.49&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. Ibid., p.29&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. Ibid.&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-4&quot;&gt;4. Ibid&lt;a href=&quot;#fnref-4&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-5&quot;&gt;5. Ibid., p.49&lt;a href=&quot;#fnref-5&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-6&quot;&gt;6. Ibid.&lt;a href=&quot;#fnref-6&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-7&quot;&gt;7. Ibid., p.53&lt;a href=&quot;#fnref-7&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-8&quot;&gt;8. Ibid., p.51. There are some really good illustraions on pp.51 and 52 with some follow-up intutition on page 53 wrt to time series splits&lt;a href=&quot;#fnref-8&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-9&quot;&gt;9. Ibid., p.53&lt;a href=&quot;#fnref-9&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-10&quot;&gt;10. Ibid.&lt;a href=&quot;#fnref-10&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-11&quot;&gt;11. Ibid. pp.53-54&lt;a href=&quot;#fnref-11&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Wayde Gilliam</name></author><category term="fastai" /><category term="fastbook" /><category term="fastbook-chapter-1" /><category term="what-is" /><category term="how-to" /><category term="datasets" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ohmeow.com/images/articles/train-validation-test-sets.png" /><media:content medium="image" url="https://ohmeow.com/images/articles/train-validation-test-sets.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What is the difference between categorial and continuous datatypes</title><link href="https://ohmeow.com/what-is/categorial-continuous-datatypes" rel="alternate" type="text/html" title="What is the difference between categorial and continuous datatypes" /><published>2021-11-04T00:00:00-05:00</published><updated>2021-11-04T00:00:00-05:00</updated><id>https://ohmeow.com/what-is/what-is-categorial-continuous-datatypes</id><content type="html" xml:base="https://ohmeow.com/what-is/categorial-continuous-datatypes">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-11-04-what-is-categorial-continuous-datatypes.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When it comes to both your inputs and targets, knowing whether they are categorial or continuous guides how you represent them, the loss function you use, and the metrics you choose in measuring performance.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-is-a-categorical-datatype?&quot;&gt;What is a categorical datatype?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-categorical-datatype?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Categorical&lt;/em&gt;&lt;/strong&gt; data &quot;contains values that are one of a discrete set of choice&quot; such as gender, occupation, day of week, etc... &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3 id=&quot;What-if-our-target-is-categorical?&quot;&gt;What if our &lt;strong&gt;target&lt;/strong&gt; is categorical?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-if-our-target-is-categorical?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;If your target/lables are categorical, then you have either a &lt;strong&gt;multi-classification classification&lt;/strong&gt; problem (e.g., you are trying to predict a single class) or a &lt;strong&gt;multi-label classification problem&lt;/strong&gt; (e.g., you are trying to predict whether your example belongs to zero or multiple classes).&lt;/p&gt;
&lt;h4 id=&quot;Multi-classification-tasks&quot;&gt;Multi-classification tasks&lt;a class=&quot;anchor-link&quot; href=&quot;#Multi-classification-tasks&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;For multi-classification tasks, a sensible loss function would be &lt;a href=&quot;https://ohmeow.com/posts/2020/04/04/understanding-cross-entropy-loss.html&quot;&gt;cross entropy loss&lt;/a&gt; (&lt;code&gt;nn.CrossEntropyLoss&lt;/code&gt;) and &lt;a href=&quot;https://ohmeow.com/what-is/a-metric#Metrics-to-use-based-on-task&quot;&gt;useful metrics&lt;/a&gt; are likely to include error rate, accuracy, F1, recall, and/or precision depending on your business objectices and the make up of your dataset.  For example, if you're dealing with a highly imbalanced dataset, choosing accuracy would lead to an inflated sense of model performance since it may be learning to just predict the most common class.
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;What if you need to predict &quot;None&quot;? This is more real world and covered nicely in Zach Mueller&amp;#8217;s &lt;a href=&quot;https://walkwithfastai.com/Unknown_Labels&quot;&gt;Recognizing Unknown Images (or the Unknown Label problem)&lt;/a&gt;.
&lt;/div&gt;&lt;/p&gt;
&lt;h4 id=&quot;Multi-label-tasks&quot;&gt;Multi-label tasks&lt;a class=&quot;anchor-link&quot; href=&quot;#Multi-label-tasks&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;For multi-label tasks, a sensible loss function would be binary cross entropy loss (BCE) (&lt;code&gt;nn.BCEWithLogitsLoss&lt;/code&gt;) and useful metrics are likely to include F1, recall, and/or precision depending on your business objectices and the make up of your dataset. Notice that I didn't include error rate, or its opposite accuracy, as their datasets are generally highly imbalanced.&lt;/p&gt;
&lt;h3 id=&quot;What-if-our-input-is-categorical?&quot;&gt;What if our &lt;strong&gt;input&lt;/strong&gt; is categorical?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-if-our-input-is-categorical?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Categorical inputs are generally represented by an &lt;strong&gt;embedding&lt;/strong&gt; (e.g., a vector of numbers). &lt;strong&gt;&lt;em&gt;Why?&lt;/em&gt;&lt;/strong&gt; Mostly because it gives your model the ability to provide a more complex representation of your category than a single numer would.&lt;/p&gt;
&lt;p&gt;For example, imagine that one of your inputs is day of week (e.g., Sunday, Monday, etc.) ... what does that mean? When combined with other inputs, its likely that the meaning of it is going to be much more nuanced than a single number can represent, and so we'd like to use multiple learned numbers.  This is what an embedding is.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-is-a-continuous-datatype?&quot;&gt;What is a continuous datatype?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-continuous-datatype?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Continuous&lt;/em&gt;&lt;/strong&gt; data is numerical that represents a quantity such as age, salary, prices, etc...&lt;/p&gt;
&lt;h3 id=&quot;What-if-our-target-is-continuous?&quot;&gt;What if our &lt;strong&gt;target&lt;/strong&gt; is continuous?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-if-our-target-is-continuous?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;If your target/labels are continuous, then you have a regression problem and the most likely loss function you would choose would be mean-square-error loss (MSE) (&lt;code&gt;nn.MSELoss&lt;/code&gt;)  and your metric MSE as well&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;... MSE is already a a useful metric for this task (although its' probably more interpretable after we take the square root)&quot; ... the &lt;strong&gt;RMSE&lt;/strong&gt; (% fn 3 %}&lt;/p&gt;
&lt;p&gt;Note:For tasks that predict a continuous number, consider using &lt;code&gt;y_range&lt;/code&gt; to constrain the network to predicting a value in the known range of valid values.&lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;What-if-our-input-is-continuous?&quot;&gt;What if our &lt;strong&gt;input&lt;/strong&gt; is continuous?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-if-our-input-is-continuous?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In many cases there isn't anything special you need to do, in others, it makes sense to scale these numbers so they are in the same range (usually 0 to 1) as the rest of your continuous inputs.  This process is called &lt;strong&gt;normalization&lt;/strong&gt;. &lt;sup id=&quot;fnref-4&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. The reason you would want to do this is so continuous values with bigger range of values (say 1000) don't drown out those with a smaller range (say 5) during model training.&lt;/p&gt;
&lt;h4 id=&quot;Normalization&quot;&gt;Normalization&lt;a class=&quot;anchor-link&quot; href=&quot;#Normalization&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&quot;When training a model, if helps if your input data is &lt;em&gt;normalizaed&lt;/em&gt; - that is, has a mean of 0 and a standard deviation of 1.
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;See &lt;a href=&quot;https://towardsdatascience.com/how-to-calculate-the-mean-and-standard-deviation-normalizing-datasets-in-pytorch-704bd7d05f4c&quot;&gt;How To Calculate the Mean and Standard Deviation — Normalizing Datasets in Pytorch&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Example 1&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Some raw values: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 1. calculate their mean and standard deviation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Their mean is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; and their standard deviation is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 2. normalize their values &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;normalized&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Here are their values after normalization: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalized&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Example 2&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Some raw values: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 1. calculate their mean and standard deviation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Their mean is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; and their standard deviation is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 2. normalize their values &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;normalized&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Here are their values after normalization: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalized&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Example 1
Some raw values: tensor([  0.,  50., 100.], dtype=torch.float64)
Their mean is 50.0 and their standard deviation is 50.0
Here are their values after normalization: tensor([-1.,  0.,  1.], dtype=torch.float64)

Example 2
Some raw values: tensor([    0.,  5000., 10000.], dtype=torch.float64)
Their mean is 5000.0 and their standard deviation is 5000.0
Here are their values after normalization: tensor([-1.,  0.,  1.], dtype=torch.float64)

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;fastai supplies a &lt;code&gt;Normalize&lt;/code&gt; transform you can use to do this ... &quot;it acts on a whole mini-batch at once, so you can add it to the &lt;code&gt;batch_tfms&lt;/code&gt; secion of your data block ... you need to pass to this transform the mean and standard deviation that you want to use. If you don't, &quot;fastai will automatically calculate them from a single batch of your data). p.241
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;&quot;This means that when you distribute a model, you need to also distribute the statistics used for normalization.&quot; (p.242)
&lt;/div&gt;&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;&quot;... if you&amp;#8217;re using a model that someon else has trained, make sure you find out what normalization statistics they used an match them&quot; (p.242)
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In &lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;The Fastbook&lt;/a&gt;&lt;/em&gt; p.46&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. Ibid. p.236. A good examle of how RMSE provides a reasonable metric for regression tasks is included on this page in reference to KeyPoint detection (e.g., detecting a point/coordinate, an x and y)&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. Ibid., p.47&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-4&quot;&gt;4. Ibid., pp.241-42, 320 includes an extended discussion of the why, how, and where &quot;normalization&quot; is needed. &lt;a href=&quot;#fnref-4&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Wayde Gilliam</name></author><category term="fastai" /><category term="fastbook" /><category term="fastbook-chapter-1" /><category term="fastbook-chapter-6" /><category term="fastbook-chapter-7" /><category term="what-is" /><category term="how-to" /><category term="datatypes" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ohmeow.com/images/articles/continuous-categorial-datatypes.png" /><media:content medium="image" url="https://ohmeow.com/images/articles/continuous-categorial-datatypes.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What is machine learning</title><link href="https://ohmeow.com/what-is/machine-learning" rel="alternate" type="text/html" title="What is machine learning" /><published>2021-11-02T00:00:00-05:00</published><updated>2021-11-02T00:00:00-05:00</updated><id>https://ohmeow.com/what-is/what-is-machine-learning</id><content type="html" xml:base="https://ohmeow.com/what-is/machine-learning">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-11-02-what-is-machine-learning.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here we look at machine learning in general (of which deep learning is a subset) as well as the process of finetuning a pretrained ML model.  When you think of deep learning ... think neural networks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ohmeow/ohmeow_website/master/images/articles/what-is-ai-ml-dl.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;A-picture&quot;&gt;A picture&lt;a class=&quot;anchor-link&quot; href=&quot;#A-picture&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://github.com/ohmeow/ohmeow_website/blob/master/_notebooks/images/ajtfb-ch-1-deep_learning_overview.png?raw=1&quot; alt=&quot;&quot; title=&quot;Credit: Fastbook p.25&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;An-explanation&quot;&gt;An explanation&lt;a class=&quot;anchor-link&quot; href=&quot;#An-explanation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;&quot;Suppowe we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignemnt so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would 'learn' from its experince&quot; - Arthur Samuel &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Architecture-vs.-model&quot;&gt;Architecture vs. model&lt;a class=&quot;anchor-link&quot; href=&quot;#Architecture-vs.-model&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;&lt;p&gt;... a &lt;strong&gt;&lt;em&gt;model&lt;/em&gt;&lt;/strong&gt; is a special kind of program:it's one that can do &lt;em&gt;many different things&lt;/em&gt;, depending &amp;gt; on the &lt;strong&gt;weights&lt;/strong&gt;. &lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The functional form of the &lt;em&gt;model&lt;/em&gt; is called its &lt;strong&gt;&lt;em&gt;architecture&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;The &lt;strong&gt;architecture&lt;/strong&gt; is &quot;the &lt;em&gt;template&lt;/em&gt; of the model that we&amp;#8217;re trying to fit; i.e., the actual mathematical function that we&amp;#8217;re passing the input data and parameters to&quot; ... whereas the &lt;strong&gt;model&lt;/strong&gt; is a particular set of parameters + the architecture.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Parameters&quot;&gt;Parameters&lt;a class=&quot;anchor-link&quot; href=&quot;#Parameters&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;Weights&lt;/strong&gt; are just variables, and a &lt;strong&gt;weight assignment&lt;/strong&gt; is a particuarl choice of values for those 
variables. [Weights] are generally referred to as model &lt;strong&gt;&lt;em&gt;parameters&lt;/em&gt;&lt;/strong&gt; ... the term &lt;em&gt;weights&lt;/em&gt; being
reserved for a particular type of model parameter. &lt;sup id=&quot;fnref-3&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;weights&lt;/em&gt; are called &lt;strong&gt;&lt;em&gt;parameters&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;These parameters are the things that are &quot;learnt&quot;; the values that can be updated, whereas &lt;strong&gt;activations&lt;/strong&gt; in a neural network are simply numbers as the result of some calculation.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Inputs-vs.labels&quot;&gt;Inputs vs.labels&lt;a class=&quot;anchor-link&quot; href=&quot;#Inputs-vs.labels&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The &lt;strong&gt;&lt;em&gt;inputs&lt;/em&gt;&lt;/strong&gt;, also known as your &lt;strong&gt;&lt;em&gt;independent variable(s)&lt;/em&gt;&lt;/strong&gt; [your &lt;code&gt;X&lt;/code&gt;] is what your model uses to make &lt;strong&gt;&lt;em&gt;predictions&lt;/em&gt;&lt;/strong&gt;. &lt;sup id=&quot;fnref-4&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;&lt;em&gt;labels&lt;/em&gt;&lt;/strong&gt;, also known as your &lt;strong&gt;&lt;em&gt;dependent variable(s)&lt;/em&gt;&lt;/strong&gt; [your &lt;code&gt;y&lt;/code&gt;] represent the correct target value for your task. &lt;sup id=&quot;fnref-5&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-5&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Loss&quot;&gt;Loss&lt;a class=&quot;anchor-link&quot; href=&quot;#Loss&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;blockquote&gt;&lt;p&gt;The [model's] measure of performance is called the &lt;strong&gt;&lt;em&gt;loss&lt;/em&gt;&lt;/strong&gt; ... [the value of which depends on how well your model is able to predict] the correct &lt;strong&gt;&lt;em&gt;labels&lt;/em&gt;&lt;/strong&gt;. &lt;sup id=&quot;fnref-6&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-6&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;strong&gt;&lt;em&gt;loss&lt;/em&gt;&lt;/strong&gt; is a measure of model performance that SGD can use to make your model better. A good loss function provides good gradients (slopes) that can be used to make even very minor changes to your weights so as to improve things. Visually, you want gentle rolling hills rather than abrupt steps or jagged peaks.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;You can think of the &lt;strong&gt;loss&lt;/strong&gt; as the model&amp;#8217;s &lt;strong&gt;metric&lt;/strong&gt;, that is, how it both understands how good it is and can help it improve.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Transfer-learning&quot;&gt;Transfer learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Transfer-learning&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Transfer learning&lt;/em&gt;&lt;/strong&gt; is the process of taking a &lt;strong&gt;&quot;pretrained model&quot;&lt;/strong&gt; that has been trained on a very large dataset with proven SOTA results, and &lt;strong&gt;&quot;fine tuning&quot;&lt;/strong&gt; it for your specific task, which while likely similar to the task the pretrained model was trained for to one degree or another, is not the necesarily the same.&lt;/p&gt;
&lt;h3 id=&quot;How-does-it-work?&quot;&gt;How does it work?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-does-it-work?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;The &lt;strong&gt;head&lt;/strong&gt; of your model (the newly added part specific to your dataset/task) should be trained first since it is the only one with completely random weights. &lt;/li&gt;
&lt;li&gt;The degree to which your weights of the pretrained model will need to be updated is proportional to how similar your data is to the data it was trained on. The more dissimilar, the more the weights will need to be changed.&lt;/li&gt;
&lt;li&gt;Your model will only be as good as the data it was trained on, so make sure what you have is representative of what it will see in the real world. It &quot;can learn to operate on only the patterns seen in the input data used to train it.&quot;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;&lt;p&gt;The process of &lt;em&gt;training&lt;/em&gt; (or &lt;em&gt;fitting&lt;/em&gt;) the model is the process of finding a set of &lt;em&gt;parameter values&lt;/em&gt; 
(or &lt;em&gt;weights&lt;/em&gt;) that specialize that general architecture into a model that works well for our 
particular kind of data [and task]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;What-is-the-high-level-approach-in-fastai?&quot;&gt;What is the high-level approach in fastai?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-the-high-level-approach-in-fastai?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;fastai provides a &lt;code&gt;fine_tune&lt;/code&gt; method that uses proven tricks and hyperparameters for various DL tasks that the author's have found works well most of the time. &lt;sup id=&quot;fnref-7&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-7&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-do-we-have-at-the-end-of-training-(or-finetuning)?&quot;&gt;What do we have at the end of training (or finetuning)?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-do-we-have-at-the-end-of-training-(or-finetuning)?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;... once the model is trained - that is, once we've chosen our final weight assignments - then we can think of the weights as being &lt;em&gt;part of the model&lt;/em&gt; since we're not varying them anymore. &lt;sup id=&quot;fnref-8&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-8&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This means a trained model can be treated like a typical function.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In &lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;The Fastbook&lt;/a&gt;&lt;/em&gt; p.21&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. Ibid.&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. Ibid., pp.21-22&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-4&quot;&gt;4. Ibid., p.22&lt;a href=&quot;#fnref-4&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-5&quot;&gt;5. Ibid.&lt;a href=&quot;#fnref-5&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-6&quot;&gt;6. Ibid.&lt;a href=&quot;#fnref-6&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-7&quot;&gt;7. Ibid., pp.32-33. Includes a full discussion on how the method works&lt;a href=&quot;#fnref-7&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-8&quot;&gt;8. Ibid., p.22&lt;a href=&quot;#fnref-8&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Wayde Gilliam</name></author><category term="fastai" /><category term="fastbook" /><category term="fastbook-chapter-1" /><category term="what-is" /><category term="how-to" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ohmeow.com/images/articles/what-is-ai-ml-dl.jpg" /><media:content medium="image" url="https://ohmeow.com/images/articles/what-is-ai-ml-dl.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How to learn (deep learning)</title><link href="https://ohmeow.com/how-to/learn-deep-learning" rel="alternate" type="text/html" title="How to learn (deep learning)" /><published>2021-11-01T00:00:00-05:00</published><updated>2021-11-01T00:00:00-05:00</updated><id>https://ohmeow.com/how-to/how-to-learn-deep-learning</id><content type="html" xml:base="https://ohmeow.com/how-to/learn-deep-learning">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-11-01-how-to-learn-deep-learning.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Cervantes once wrote that &quot;the journey is better than the inn&quot;, but I rather like to think that the journey &lt;em&gt;is&lt;/em&gt; the inn.&lt;/p&gt;
&lt;p&gt;It means that irrespective to its difficulties (and likely because of them), your adventures is what you look back on with fondness at its end rather than the end itself.  It's why I enjoy reading &quot;The Lord of the Rings&quot; every five years or so, where as I age and experience the hand life has dealt me, I find myself appreciating different aspects of the story and gaining new insights into what I value and into what I want to be as a human being. I find my journey with deep learning to be roughly analgous to that.&lt;/p&gt;
&lt;p&gt;I've been a part of &lt;a href=&quot;https://forums.fast.ai/u/wgpubs/summary&quot;&gt;the fast.ai community&lt;/a&gt; for several years. I've been through &lt;a href=&quot;https://course.fast.ai/&quot;&gt;the course&lt;/a&gt; multiple times (since it was using theano back in the old days), I've contributed to the library, and use it as the basis for one of &lt;a href=&quot;https://ohmeow.github.io/blurr/&quot;&gt;my own&lt;/a&gt;.  And as with each course, a re-reading of the book brings new insights, ideas, and revelations.&lt;/p&gt;
&lt;p&gt;And so here I begin with my meandering thoughts and reflections from yet another reading of what I consider &quot;The Lord of the Rings&quot; of deep learning. As such, it makes sense to being with the most foundational topic in fastbook, &lt;strong&gt;&quot;How do you learn Deep Learning?&quot;&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;You-can-do-this!&quot;&gt;You can do this!&lt;a class=&quot;anchor-link&quot; href=&quot;#You-can-do-this!&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Hi, everybody; I'm Jeremy ... I do not have any formal technical education ... didn't have great grades.
I was much more interested in doing real projects. &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is meaningful to me as someone with a BA in History and a MA in Theology. It's a reminder that if you want something, it's within your grasp to make it happen if you are willing to put in the work. It's also a reminder that key to getting there is actually doing something!  If find too many people thinking that if they just get into that school, or if they can just take that class, then they'll be a good software enginner or deep learning practitioner.  The reality is that the &lt;em&gt;only&lt;/em&gt; way you get there is by doing it ... just like pull-ups (which aren't much fun when you're starting out and/or you're 49 and overweight).&lt;/p&gt;
&lt;h2 id=&quot;The-problem-with-traditional-education&quot;&gt;The problem with traditional education&lt;a class=&quot;anchor-link&quot; href=&quot;#The-problem-with-traditional-education&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;... how math is taught - we require students to spend years doing rote memorization and learning dry
disconnected &lt;em&gt;fundatmentals&lt;/em&gt; that we claim will pay off later, long after most of them quit the subject. &lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This also is the problem with higher education in general, where young people spend at least four to five years learning things they already learned in High School or else things they don't really care about and will be forgotten right after finals, spending in excess of $100,000 for the privilege of it and likely going into debt in the tens of thousands of dollars, all with this idea that having done it they will be prepared for the real world.  Unfortunately, that's not how it works. Whether you are in a university of even go to university, what matter is what you do ... not what classes you took or what your GPA is.&lt;/p&gt;
&lt;h2 id=&quot;Deep-Learning-(and-coding-in-general)-is-an-art-maybe-more-so-than-a-science&quot;&gt;Deep Learning (and coding in general) is an art maybe more so than a science&lt;a class=&quot;anchor-link&quot; href=&quot;#Deep-Learning-(and-coding-in-general)-is-an-art-maybe-more-so-than-a-science&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;The hardest part of deep learning is artisanal. &lt;sup id=&quot;fnref-3&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I remember going to an iOS conference way back in the day and a conference speaker asking how many folks in the session I was sitting in had a background in music. 80-90% of the audience raised their hands.  Sure, there is math and stats and a science to deep learning, but like any coding enterprise, it's an art ... with some artists being better than others along with room for improvement regardless of whether you're Van Gough or painting by the numbers.&lt;/p&gt;
&lt;h2 id=&quot;Doing-is-how-you-learn,-and-what-you've-done-is-what-matters&quot;&gt;Doing is how you learn, and what you've done is what matters&lt;a class=&quot;anchor-link&quot; href=&quot;#Doing-is-how-you-learn,-and-what-you've-done-is-what-matters&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;... focus on your hobbies and passions ... Common character traits in the people who do well at deep learning include playfulness and curiosity. &lt;sup id=&quot;fnref-4&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;at Tesla .. CEO Elon Musk says 'A PhD is definitely not required. All that matters is a deep understanding of AI &amp;amp; ability to implement NNs in a way that is actually useful .... Don't care if you even graduated High School.' &lt;sup id=&quot;fnref-5&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-5&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;... the most important thing for learning deep learning is writing code and experimenting.&quot; &lt;sup id=&quot;fnref-6&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-6&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Folks-to-follow&quot;&gt;Folks to follow&lt;a class=&quot;anchor-link&quot; href=&quot;#Folks-to-follow&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;It's always helpful to have some role models; folks who practice the lessons learned above and can help you along your journey.&lt;/p&gt;
&lt;p&gt;For starters, consider this image of the top 12 users based on most likes in the fast.ai forums:
&lt;img src=&quot;https://github.com/ohmeow/ohmeow_website/blob/master/images/articles/20211102-fastai-forums-top-12.png?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Aside from the founders of &lt;a href=&quot;https://www.fast.ai/&quot;&gt;fast.ai&lt;/a&gt; and a bunch of them working for noteable ML companies like &lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt; and &lt;a href=&quot;https://wandb.ai/site&quot;&gt;Weights &amp;amp; Biases&lt;/a&gt;, I can think of at least &lt;strong&gt;&lt;em&gt;FOUR&lt;/em&gt;&lt;/strong&gt; things these folks have in common:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;They are &lt;strong&gt;fearless in asking what they may have even considered, dumb questions&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;They are &lt;strong&gt;active in researching the answers to their own questions&lt;/strong&gt; (even the dumb ones) and those asked by others.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;They are &lt;strong&gt;active in teaching&lt;/strong&gt; others through blogs, books, open source libraries, study groups, and podcasts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;They build&lt;/strong&gt; things! That is, they all have experience building models and making them usable via deployed applications and/or in kaggle compeititions.  Anyone can bake a half-cooked model in a Jupyter notebook, but few can turn it into something others can use.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These traits aren't just key to learning deep learning; they are key to learning anything!  Practice them and you guarantee yourself success in learning anything you've set your mind on.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-if-I-can-only-follow-three?&quot;&gt;What if I can only follow three?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-if-I-can-only-follow-three?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Aside from Jeremy (&lt;a href=&quot;https://twitter.com/jeremyphoward&quot;&gt;@jeremyphoward&lt;/a&gt;), who's a given, if I could only follow three people who have mastered to art of learning deep learning, they would be ...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Radek Osmulsk&lt;/strong&gt;: (twitter: &lt;a href=&quot;https://twitter.com/radekosmulski&quot;&gt;@radekosmulski&lt;/a&gt;)

&lt;center&gt;
    &lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;If you found this of value, you might be interested in a book on learning deep learning that I wrote&lt;br /&gt;&lt;br /&gt;check it out here &amp;gt;&amp;gt;&amp;gt; &lt;a href=&quot;https://t.co/ApKlm8BRmy&quot;&gt;https://t.co/ApKlm8BRmy&lt;/a&gt;&lt;/p&gt;&amp;mdash; Radek Osmulski (@radekosmulski) &lt;a href=&quot;https://twitter.com/radekosmulski/status/1455527697661169664?ref_src=twsrc%5Etfw&quot;&gt;November 2, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zach Mueller&lt;/strong&gt;: (twitter: &lt;a href=&quot;https://twitter.com/TheZachMueller&quot;&gt;@TheZachMueller&lt;/a&gt;)

&lt;center&gt;
    &lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;To me, I think it boiled down to how I learned. I took those two courses essentially over the course of a year or so. Approaching each lesson slowly, and letting myself wander in the related concepts, learning as much as I could through online communities.&lt;/p&gt;&amp;mdash; Zach Mueller (@TheZachMueller) &lt;a href=&quot;https://twitter.com/TheZachMueller/status/1451941577841127433?ref_src=twsrc%5Etfw&quot;&gt;October 23, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sanyam Bhutani&lt;/strong&gt;: (twitter: &lt;a href=&quot;https://twitter.com/bhutanisanyam1&quot;&gt;@bhutanisanyam1&lt;/a&gt;)

&lt;center&gt;
    &lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;The &lt;a href=&quot;https://twitter.com/PyTorch?ref_src=twsrc%5Etfw&quot;&gt;@PyTorch&lt;/a&gt; book reading group &lt;a href=&quot;https://twitter.com/weights_biases?ref_src=twsrc%5Etfw&quot;&gt;@weights_biases&lt;/a&gt; comes to an end🙏&lt;br /&gt;&lt;br /&gt;We had an incredible 10 weeks of learning!&lt;br /&gt;&lt;br /&gt;As a group wanted to extend our gratitude to the incredible authors: Eli, &lt;a href=&quot;https://twitter.com/lantiga?ref_src=twsrc%5Etfw&quot;&gt;@lantiga&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://twitter.com/ThomasViehmann?ref_src=twsrc%5Etfw&quot;&gt;@ThomasViehmann&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;A few words from our community:&lt;a href=&quot;https://t.co/3ODz6J1vad&quot;&gt;https://t.co/3ODz6J1vad&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sanyam Bhutani (@bhutanisanyam1) &lt;a href=&quot;https://twitter.com/bhutanisanyam1/status/1452599997493481472?ref_src=twsrc%5Etfw&quot;&gt;October 25, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;p&gt;Personally, I &lt;strong&gt;do&lt;/strong&gt; follow each of these individuals on twitter and you should too! Though I've never met any of them IRL, I consider the colleagues, friends, and amongst the most helpful for those looking to get started in machine learning.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;Twitter is imo the best place to network with fellow ML/DL practioners and stay up-to-date with the latest developments in ML in general
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;What-if-I'm-too-lazy-to-read-any-of-that-stuff-above?&quot;&gt;What if I'm too lazy to read any of that stuff above?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-if-I'm-too-lazy-to-read-any-of-that-stuff-above?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Well, you can watch this video (and then go back and read that stuff anyways) ...

&lt;center class=&quot;youtube-iframe-wrapper&quot;&gt;
    &lt;iframe width=&quot;730&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/_QUEXsHfsA0?t=935&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In &lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;The Fastbook&lt;/a&gt;&lt;/em&gt; p.8&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. Ibid., p.9&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. Ibid., p.10&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-4&quot;&gt;4. Ibid., p.11&lt;a href=&quot;#fnref-4&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-5&quot;&gt;5. Ibid.&lt;a href=&quot;#fnref-5&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-6&quot;&gt;6. Ibid.&lt;a href=&quot;#fnref-6&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Wayde Gilliam</name></author><category term="fastai" /><category term="fastbook" /><category term="fastbook-chapter-1" /><category term="how-to" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ohmeow.com/images/articles/thumbs_up_kid.jpeg" /><media:content medium="image" url="https://ohmeow.com/images/articles/thumbs_up_kid.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>